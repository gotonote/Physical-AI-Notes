# å¼ºåŒ–å­¦ä¹ åŸºç¡€

> Reinforcement Learning Fundamentals for Robotics

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ç‰©ç†AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œè®©æœºå™¨äººèƒ½å¤Ÿé€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’è‡ªä¸»å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚

---

## ğŸ“‹ ç›®å½•

1. [å¼ºåŒ–å­¦ä¹ æ¦‚è¿°](#1-å¼ºåŒ–å­¦ä¹ æ¦‚è¿°)
2. [MDPåŸºç¡€](#2-mdpåŸºç¡€)
3. [Value-basedæ–¹æ³•](#3-value-basedæ–¹æ³•)
4. [Policy-basedæ–¹æ³•](#4-policy-basedæ–¹æ³•)
5. [Actor-Criticæ–¹æ³•](#5-actor-criticæ–¹æ³•)
6. [ä»£ç å®ç°](#6-ä»£ç å®ç°)
7. [å®æˆ˜ç»ƒä¹ ](#7-å®æˆ˜ç»ƒä¹ )

---

## 1. å¼ºåŒ–å­¦ä¹ æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               å¼ºåŒ–å­¦ä¹ æ¡†æ¶                               â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚     â”‚  Agent  â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ç¯å¢ƒ   â”‚                    â”‚
â”‚     â”‚  æ™ºèƒ½ä½“  â”‚  åŠ¨ä½œa  â”‚ Environ â”‚                    â”‚
â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â”‚
â”‚          â”‚                   â”‚                         â”‚
â”‚          â”‚     çŠ¶æ€s         â”‚                         â”‚
â”‚          â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚
â”‚          â”‚                   â”‚                         â”‚
â”‚          â”‚     å¥–åŠ±r         â”‚                         â”‚
â”‚          â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚
â”‚          â”‚                   â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ ¸å¿ƒè¦ç´ ï¼š
- State (s): ç¯å¢ƒçŠ¶æ€
- Action (a): æ™ºèƒ½ä½“åŠ¨ä½œ
- Reward (r): å³æ—¶å¥–åŠ±ä¿¡å·
- Policy (Ï€): çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„
- Value (V): çŠ¶æ€/åŠ¨ä½œçš„é•¿æœŸä»·å€¼
```

### 1.2 ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«

| ç‰¹æ€§ | ç›‘ç£å­¦ä¹  | å¼ºåŒ–å­¦ä¹  |
|------|---------|---------|
| æ•°æ®æ¥æº | é™æ€æ ‡æ³¨æ•°æ® | ç¯å¢ƒäº¤äº’äº§ç”Ÿ |
| åé¦ˆæ—¶æœº | å³æ—¶ | å»¶è¿Ÿ |
| ç›®æ ‡ | æ‹Ÿåˆæ ‡ç­¾ | æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ± |
| æ•°æ®åˆ†å¸ƒ | å›ºå®š | å—ç­–ç•¥å½±å“ |

### 1.3 åœ¨æœºå™¨äººä¸­çš„åº”ç”¨

```
åº”ç”¨åœºæ™¯ï¼š
â”œâ”€â”€ æœºæ¢°è‡‚æŠ“å–
â”‚   â””â”€ å­¦ä¹ æœ€ä¼˜æŠ“å–ç­–ç•¥
â”œâ”€â”€ è¶³å¼æœºå™¨äººè¡Œèµ°
â”‚   â””â”€ å­¦ä¹ ç¨³å®šæ­¥æ€
â”œâ”€â”€ è‡ªåŠ¨é©¾é©¶
â”‚   â””â”€ å­¦ä¹ é©¾é©¶å†³ç­–
â””â”€â”€ æ¸¸æˆAI
    â””â”€ å­¦ä¹ æ¸¸æˆç­–ç•¥
```

---

## 2. MDPåŸºç¡€

### 2.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

**å®šä¹‰**ï¼šMDPç”±äº”å…ƒç»„ $(S, A, P, R, \gamma)$ ç»„æˆ

- $S$: çŠ¶æ€ç©ºé—´
- $A$: åŠ¨ä½œç©ºé—´
- $P(s'|s,a)$: çŠ¶æ€è½¬ç§»æ¦‚ç‡
- $R(s,a,s')$: å¥–åŠ±å‡½æ•°
- $\gamma$: æŠ˜æ‰£å› å­

### 2.2 è´å°”æ›¼æ–¹ç¨‹

**çŠ¶æ€ä»·å€¼å‡½æ•°**ï¼š
$$V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s \right]$$

**è´å°”æ›¼æœŸæœ›æ–¹ç¨‹**ï¼š
$$V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$$

**è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹**ï¼š
$$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')]$$

### 2.3 æœºå™¨äººMDPç¤ºä¾‹

```python
import numpy as np
from typing import Tuple, Dict, List

class RobotArmMDP:
    """æœºæ¢°è‡‚æŠ“å–MDPç¯å¢ƒ"""
    
    def __init__(self):
        # çŠ¶æ€ç©ºé—´ï¼šæœ«ç«¯ä½ç½®(x,y,z) + ç›®æ ‡ä½ç½® + å¤¹çˆªçŠ¶æ€
        self.state_dim = 7
        
        # åŠ¨ä½œç©ºé—´ï¼šä½ç½®å¢é‡(Î”x,Î”y,Î”z) + å¤¹çˆªå¼€åˆ
        self.action_dim = 4
        
        # æŠ˜æ‰£å› å­
        self.gamma = 0.99
        
        # ç¯å¢ƒå‚æ•°
        self.target_pos = np.array([0.5, 0.0, 0.3])
        self.gripper_state = 1.0  # 1.0=å¼€, 0.0=å…³
        
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.gripper_pos = np.array([0.0, 0.0, 0.5])
        self.gripper_state = 1.0
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        return np.concatenate([
            self.gripper_pos,
            self.target_pos,
            [self.gripper_state]
        ])
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›(æ–°çŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»ˆæ­¢, ä¿¡æ¯)"""
        
        # æ›´æ–°ä½ç½®ï¼ˆé™åˆ¶èŒƒå›´ï¼‰
        delta = action[:3] * 0.05  # ç¼©æ”¾åŠ¨ä½œ
        self.gripper_pos = np.clip(
            self.gripper_pos + delta,
            [-1, -1, 0], [1, 1, 1]
        )
        
        # æ›´æ–°å¤¹çˆªçŠ¶æ€
        self.gripper_state = np.clip(
            self.gripper_state + action[3] * 0.1, 0, 1
        )
        
        # è®¡ç®—å¥–åŠ±
        distance = np.linalg.norm(self.gripper_pos - self.target_pos)
        reward = -distance  # è·ç¦»æƒ©ç½š
        
        # æŠ“å–æˆåŠŸå¥–åŠ±
        if distance < 0.05 and self.gripper_state < 0.2:
            reward += 10.0
            done = True
        else:
            done = False
        
        # è¶…æ—¶æƒ©ç½š
        if hasattr(self, 'steps'):
            self.steps += 1
            if self.steps > 200:
                done = True
                reward -= 5.0
        else:
            self.steps = 1
        
        return self._get_state(), reward, done, {}
```

---

## 3. Value-basedæ–¹æ³•

### 3.1 Q-Learning

**æ ¸å¿ƒæ€æƒ³**ï¼šå­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•° $Q(s,a)$

**æ›´æ–°è§„åˆ™**ï¼š
$$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

### 3.2 DQN (Deep Q-Network)

ä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque

class DQNNetwork(nn.Module):
    """DQNç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class DQNAgent:
    """DQNæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, 
                 lr: float = 1e-3, gamma: float = 0.99,
                 epsilon: float = 1.0, epsilon_min: float = 0.01,
                 epsilon_decay: float = 0.995,
                 buffer_size: int = 10000, batch_size: int = 64):
        
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        
        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQNNetwork(state_dim, action_dim)
        self.target_network = DQNNetwork(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = deque(maxlen=buffer_size)
    
    def select_action(self, state: np.ndarray) -> int:
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_network(state_tensor)
            return q_values.argmax(dim=1).item()
    
    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.append((state, action, reward, next_state, done))
    
    def train(self):
        """è®­ç»ƒä¸€æ­¥"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·æ‰¹æ¬¡
        batch = random.sample(self.replay_buffer, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(dones)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆDouble DQNï¼‰
        with torch.no_grad():
            next_actions = self.q_network(next_states).argmax(dim=1)
            next_q = self.target_network(next_states).gather(
                1, next_actions.unsqueeze(1)
            ).squeeze()
            target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # è®¡ç®—æŸå¤±å¹¶æ›´æ–°
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ›´æ–°Îµ
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
```

### 3.3 DQNå˜ä½“

| æ–¹æ³• | æ”¹è¿›ç‚¹ | æ ¸å¿ƒæ€æƒ³ |
|------|--------|---------|
| Double DQN | å‡å°‘è¿‡ä¼°è®¡ | åˆ†ç¦»åŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼° |
| Dueling DQN | æ›´å¥½ä»·å€¼ä¼°è®¡ | åˆ†ç¦»çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿ |
| Prioritized ER | æ ·æœ¬æ•ˆç‡ | ä¼˜å…ˆé‡‡æ ·é«˜TDè¯¯å·®æ ·æœ¬ |
| Rainbow | ç»¼åˆæ”¹è¿› | æ•´åˆå¤šç§æ”¹è¿›æŠ€æœ¯ |

---

## 4. Policy-basedæ–¹æ³•

### 4.1 ç­–ç•¥æ¢¯åº¦

**æ ¸å¿ƒæ€æƒ³**ï¼šç›´æ¥ä¼˜åŒ–ç­–ç•¥ $\pi_\theta(a|s)$

**ç›®æ ‡å‡½æ•°**ï¼š
$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]$$

**ç­–ç•¥æ¢¯åº¦å®šç†**ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$$

### 4.2 REINFORCE

```python
class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼ˆç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # å‡å€¼å’Œæ ‡å‡†å·®è¾“å‡º
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
    
    def forward(self, x: torch.Tensor):
        features = self.shared(x)
        mean = self.mean_head(features)
        std = torch.exp(self.log_std.clamp(-20, 2))
        return mean, std
    
    def get_action(self, state: np.ndarray):
        """é‡‡æ ·åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            mean, std = self.forward(state_tensor)
            dist = torch.distributions.Normal(mean, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1)
        return action.squeeze().numpy(), log_prob.item()


class REINFORCEAgent:
    """REINFORCEæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3, gamma: float = 0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
        # å­˜å‚¨ä¸€æ¡è½¨è¿¹
        self.log_probs = []
        self.rewards = []
    
    def select_action(self, state: np.ndarray) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        mean, std = self.policy(state_tensor)
        dist = torch.distributions.Normal(mean, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1)
        
        self.log_probs.append(log_prob)
        return action.squeeze().numpy()
    
    def store_reward(self, reward: float):
        """å­˜å‚¨å¥–åŠ±"""
        self.rewards.append(reward)
    
    def update(self):
        """ç­–ç•¥æ›´æ–°"""
        # è®¡ç®—æŠ˜æ‰£ç´¯ç§¯å¥–åŠ±
        returns = []
        G = 0
        for r in reversed(self.rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # æ ‡å‡†åŒ–
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
        log_probs = torch.cat(self.log_probs)
        loss = -(log_probs * returns).mean()
        
        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ¸…ç©ºå­˜å‚¨
        self.log_probs = []
        self.rewards = []
        
        return loss.item()
```

### 4.3 PPO (Proximal Policy Optimization)

PPOæ˜¯å½“å‰æœ€æµè¡Œçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¹‹ä¸€ã€‚

```python
class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int,
                 lr: float = 3e-4, gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 clip_epsilon: float = 0.2,
                 entropy_coef: float = 0.01,
                 value_coef: float = 0.5):
        
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        
        # Actor-Criticç½‘ç»œ
        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        self.optimizer = optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()),
            lr=lr
        )
    
    def compute_gae(self, rewards, values, dones, next_value):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)"""
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_val = next_value
            else:
                next_val = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_val * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        return advantages
    
    def update(self, states, actions, old_log_probs, advantages, returns, epochs=10, batch_size=64):
        """PPOæ›´æ–°"""
        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        old_log_probs = torch.FloatTensor(old_log_probs)
        advantages = torch.FloatTensor(advantages)
        returns = torch.FloatTensor(returns)
        
        dataset_size = len(states)
        
        for _ in range(epochs):
            indices = np.random.permutation(dataset_size)
            
            for start in range(0, dataset_size, batch_size):
                end = start + batch_size
                idx = indices[start:end]
                
                batch_states = states[idx]
                batch_actions = actions[idx]
                batch_old_log_probs = old_log_probs[idx]
                batch_advantages = advantages[idx]
                batch_returns = returns[idx]
                
                # è®¡ç®—æ–°çš„log_prob
                mean, std = self.actor(batch_states)
                dist = torch.distributions.Normal(mean, std)
                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)
                entropy = dist.entropy().sum(dim=-1).mean()
                
                # PPOè£å‰ªç›®æ ‡
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # ä»·å€¼å‡½æ•°æŸå¤±
                values = self.critic(batch_states).squeeze()
                value_loss = nn.MSELoss()(values, batch_returns)
                
                # æ€»æŸå¤±
                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
                
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(self.actor.parameters()) + list(self.critic.parameters()),
                    0.5
                )
                self.optimizer.step()
        
        return loss.item()
```

---

## 5. Actor-Criticæ–¹æ³•

### 5.1 A2C / A3C

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Actor-Criticæ¶æ„                        â”‚
â”‚                                                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚       State s        â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                         â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚     ç‰¹å¾æå–å™¨        â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                         â”‚                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â–¼                               â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Actor Ï€   â”‚                â”‚  Critic V   â”‚         â”‚
â”‚  â”‚   ç­–ç•¥ç½‘ç»œ   â”‚                â”‚  ä»·å€¼ç½‘ç»œ   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                              â”‚                â”‚
â”‚         â–¼                              â–¼                â”‚
â”‚    Action a                         Value V(s)          â”‚
â”‚                                                         â”‚
â”‚  ActoræŸå¤±: -log Ï€(a|s) * A(s,a)                        â”‚
â”‚  CriticæŸå¤±: (V(s) - R)Â²                                â”‚
â”‚  ä¼˜åŠ¿å‡½æ•°: A(s,a) = Q(s,a) - V(s)                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 SAC (Soft Actor-Critic)

SACæ˜¯å½“å‰æœ€å…ˆè¿›çš„è¿ç»­æ§åˆ¶ç®—æ³•ä¹‹ä¸€ï¼Œå¼•å…¥äº†ç†µæ­£åˆ™åŒ–ã€‚

```python
class SACAgent:
    """SACæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int,
                 lr: float = 3e-4, gamma: float = 0.99,
                 tau: float = 0.005, alpha: float = 0.2,
                 auto_entropy: bool = True):
        
        self.gamma = gamma
        self.tau = tau
        self.auto_entropy = auto_entropy
        
        # ç½‘ç»œ
        self.actor = GaussianActor(state_dim, action_dim)
        self.critic1 = CriticNetwork(state_dim, action_dim)
        self.critic2 = CriticNetwork(state_dim, action_dim)
        self.critic1_target = CriticNetwork(state_dim, action_dim)
        self.critic2_target = CriticNetwork(state_dim, action_dim)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr)
        
        # ç†µç³»æ•°
        if auto_entropy:
            self.target_entropy = -action_dim
            self.log_alpha = torch.zeros(1, requires_grad=True)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)
        else:
            self.alpha = alpha
    
    def update(self, batch):
        """SACæ›´æ–°"""
        states, actions, rewards, next_states, dones = batch
        
        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)
        
        # ---- æ›´æ–°Critic ----
        with torch.no_grad():
            next_actions, next_log_probs = self.actor.sample(next_states)
            q1_next = self.critic1_target(next_states, next_actions)
            q2_next = self.critic2_target(next_states, next_actions)
            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs
            target_q = rewards + self.gamma * (1 - dones) * q_next
        
        q1 = self.critic1(states, actions)
        q2 = self.critic2(states, actions)
        
        critic1_loss = nn.MSELoss()(q1, target_q)
        critic2_loss = nn.MSELoss()(q2, target_q)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()
        
        # ---- æ›´æ–°Actor ----
        new_actions, log_probs = self.actor.sample(states)
        q1_new = self.critic1(states, new_actions)
        q2_new = self.critic2(states, new_actions)
        q_new = torch.min(q1_new, q2_new)
        
        actor_loss = (self.alpha * log_probs - q_new).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # ---- æ›´æ–°Alpha ----
        if self.auto_entropy:
            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            self.alpha = self.log_alpha.exp().item()
        
        # ---- è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ ----
        for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        
        for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        
        return {
            'critic_loss': (critic1_loss.item() + critic2_loss.item()) / 2,
            'actor_loss': actor_loss.item(),
            'alpha': self.alpha
        }
```

### 5.3 TD3 (Twin Delayed DDPG)

TD3è§£å†³äº†DDPGçš„Qå€¼è¿‡ä¼°è®¡é—®é¢˜ã€‚

**ä¸‰ä¸ªæ ¸å¿ƒæ”¹è¿›**ï¼š
1. **Twin Critics** - ä½¿ç”¨ä¸¤ä¸ªCriticå–æœ€å°å€¼
2. **Delayed Policy Updates** - å»¶è¿Ÿæ›´æ–°Actor
3. **Target Policy Smoothing** - ç›®æ ‡ç­–ç•¥å¹³æ»‘

---

## 6. ä»£ç å®ç°

### å®Œæ•´è®­ç»ƒè„šæœ¬

```python
import gymnasium as gym
import numpy as np
from tqdm import tqdm

def train_dqn(env_name: str = "CartPole-v1", 
              num_episodes: int = 1000,
              target_update_freq: int = 10):
    """DQNè®­ç»ƒè„šæœ¬"""
    
    env = gym.make(env_name)
    agent = DQNAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n
    )
    
    rewards_history = []
    
    for episode in tqdm(range(num_episodes)):
        state, _ = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            action = agent.select_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            agent.store_transition(state, action, reward, next_state, done)
            agent.train()
            
            state = next_state
            total_reward += reward
        
        rewards_history.append(total_reward)
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % target_update_freq == 0:
            agent.update_target_network()
        
        # æ‰“å°è¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_history[-100:])
            print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}")
    
    return agent, rewards_history


def train_ppo(env_name: str = "HalfCheetah-v4",
              num_iterations: int = 1000,
              steps_per_iter: int = 2048):
    """PPOè®­ç»ƒè„šæœ¬"""
    
    env = gym.make(env_name)
    agent = PPOAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.shape[0]
    )
    
    rewards_history = []
    
    for iteration in tqdm(range(num_iterations)):
        # æ”¶é›†æ•°æ®
        states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []
        state, _ = env.reset()
        
        for _ in range(steps_per_iter):
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            
            with torch.no_grad():
                mean, std = agent.actor(state_tensor)
                dist = torch.distributions.Normal(mean, std)
                action = dist.sample()
                log_prob = dist.log_prob(action).sum(dim=-1)
                value = agent.critic(state_tensor)
            
            action_np = action.squeeze().numpy()
            next_state, reward, terminated, truncated, _ = env.step(action_np)
            done = terminated or truncated
            
            states.append(state)
            actions.append(action_np)
            rewards.append(reward)
            dones.append(done)
            log_probs.append(log_prob.item())
            values.append(value.item())
            
            state = next_state if not done else env.reset()[0]
        
        # è®¡ç®—GAEå’Œreturns
        with torch.no_grad():
            next_value = agent.critic(torch.FloatTensor(state).unsqueeze(0)).item()
        
        advantages = agent.compute_gae(rewards, values, dones, next_value)
        returns = [a + v for a, v in zip(advantages, values)]
        
        # æ›´æ–°ç­–ç•¥
        agent.update(states, actions, log_probs, advantages, returns)
        
        avg_reward = np.mean(rewards)
        rewards_history.append(avg_reward)
        
        if (iteration + 1) % 50 == 0:
            print(f"Iteration {iteration+1}, Avg Reward: {avg_reward:.2f}")
    
    return agent, rewards_history


if __name__ == "__main__":
    # è®­ç»ƒDQN
    print("Training DQN on CartPole...")
    dqn_agent, dqn_rewards = train_dqn()
    
    # è®­ç»ƒPPO
    print("\nTraining PPO on HalfCheetah...")
    ppo_agent, ppo_rewards = train_ppo()
```

---

## 7. å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå®ç°ä¸€ä¸ªç®€å•çš„Q-Learning

**ä»»åŠ¡**ï¼šåœ¨FrozenLakeç¯å¢ƒä¸­å®ç°Q-Learning

```python
# ç»ƒä¹ æ¡†æ¶
import gymnasium as gym
import numpy as np

def q_learning():
    env = gym.make("FrozenLake-v1")
    
    # åˆå§‹åŒ–Qè¡¨
    Q = np.zeros((env.observation_space.n, env.action_space.n))
    
    # TODO: å®ç°Q-Learningç®—æ³•
    # 1. Îµ-è´ªå©ªåŠ¨ä½œé€‰æ‹©
    # 2. Qè¡¨æ›´æ–°
    # 3. è®­ç»ƒå¾ªç¯
    
    pass

# å®Œæˆåæµ‹è¯•
if __name__ == "__main__":
    q_learning()
```

### ç»ƒä¹ 2ï¼šä»DQNåˆ°Double DQN

**ä»»åŠ¡**ï¼šå°†ä¸Šé¢çš„DQNä»£ç ä¿®æ”¹ä¸ºDouble DQN

**æç¤º**ï¼šåœ¨è®¡ç®—ç›®æ ‡Qå€¼æ—¶ï¼Œä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼

### ç»ƒä¹ 3ï¼šPPOå®ç°ç»†èŠ‚

**ä»»åŠ¡**ï¼šå®ç°å®Œæ•´çš„PPOç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š
1. GAEè®¡ç®—
2. ä¼˜åŠ¿å‡½æ•°æ ‡å‡†åŒ–
3. å¤šepochå°æ‰¹æ¬¡æ›´æ–°
4. æ¢¯åº¦è£å‰ª

### ç»ƒä¹ 4ï¼šæœºå™¨äººä»»åŠ¡

**ä»»åŠ¡**ï¼šåœ¨MuJoCoç¯å¢ƒä¸­è®­ç»ƒä¸€ä¸ªæœºå™¨äººæ§åˆ¶ç­–ç•¥

```python
import gymnasium as gym

# ä½¿ç”¨HalfCheetahæˆ–Antç¯å¢ƒ
env = gym.make("HalfCheetah-v4")

# TODO: 
# 1. å®ç°SACæˆ–PPOç®—æ³•
# 2. è®­ç»ƒç›´åˆ°å¹³å‡å¥–åŠ± > 3000
# 3. åˆ†æå­¦ä¹ æ›²çº¿
```

---

## ğŸ“š æ¨èèµ„æº

### ç»å…¸è®ºæ–‡
- **DQN**: Playing Atari with Deep RL (Mnih et al., 2015)
- **PPO**: Proximal Policy Optimization Algorithms (Schulman et al., 2017)
- **SAC**: Soft Actor-Critic (Haarnoja et al., 2018)
- **TD3**: Twin Delayed DDPG (Fujimoto et al., 2018)

### åœ¨çº¿è¯¾ç¨‹
- [DeepMind RL Course](https://www.deepmind.com/learning-resources)
- [Stanford CS234](http://web.stanford.edu/class/cs234/)
- [Berkeley CS285](http://rail.eecs.berkeley.edu/deeprlcourse/)

### å¼€æºå®ç°
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3)
- [CleanRL](https://github.com/vwxyzjn/cleanrl)
- [RLlib](https://docs.ray.io/en/latest/rllib/)

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ï¼Œæ¬¢è¿åé¦ˆå’Œå»ºè®®ï¼*
