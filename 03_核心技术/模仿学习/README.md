# æ¨¡ä»¿å­¦ä¹ 

> Imitation Learning for Robotics

æ¨¡ä»¿å­¦ä¹ è®©æœºå™¨äººä»äººç±»ç¤ºèŒƒä¸­å­¦ä¹ è¡Œä¸ºç­–ç•¥ï¼Œæ˜¯ç‰©ç†AIé¢†åŸŸæœ€å®ç”¨çš„å­¦ä¹ æ–¹æ³•ä¹‹ä¸€ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ¨¡ä»¿å­¦ä¹ æ¦‚è¿°](#1-æ¨¡ä»¿å­¦ä¹ æ¦‚è¿°)
2. [è¡Œä¸ºå…‹éš†](#2-è¡Œä¸ºå…‹éš†)
3. [DAggerç®—æ³•](#3-daggerç®—æ³•)
4. [GAIL](#4-gail)
5. [ä»£ç å®ç°](#5-ä»£ç å®ç°)
6. [å®æˆ˜æ¡ˆä¾‹](#6-å®æˆ˜æ¡ˆä¾‹)

---

## 1. æ¨¡ä»¿å­¦ä¹ æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ€æƒ³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¨¡ä»¿å­¦ä¹ æ¡†æ¶                              â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  äººç±»ä¸“å®¶  â”‚ â”€â”€â”€â–¶  â”‚  ç¤ºèŒƒæ•°æ®  â”‚ â”€â”€â”€â–¶  â”‚  å­¦ä¹ ç­–ç•¥  â”‚     â”‚
â”‚   â”‚  Expert   â”‚        â”‚ Demo Data â”‚        â”‚  Policy  â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â”‚   ç¤ºèŒƒæ•°æ® = {(sâ‚,aâ‚), (sâ‚‚,aâ‚‚), ..., (sâ‚™,aâ‚™)}              â”‚
â”‚                                                             â”‚
â”‚   ç›®æ ‡ï¼šå­¦ä¹ ç­–ç•¥ Ï€(a|s) ä½¿å…¶è¡Œä¸ºæ¥è¿‘ä¸“å®¶                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ä¸å¼ºåŒ–å­¦ä¹ çš„å¯¹æ¯”

| ç‰¹æ€§ | å¼ºåŒ–å­¦ä¹  | æ¨¡ä»¿å­¦ä¹  |
|------|---------|---------|
| å­¦ä¹ ä¿¡å· | å¥–åŠ±å‡½æ•° | ä¸“å®¶ç¤ºèŒƒ |
| æ¢ç´¢éœ€æ±‚ | éœ€è¦æ¢ç´¢ | æ— éœ€æ¢ç´¢ |
| æ ·æœ¬æ•ˆç‡ | è¾ƒä½ | è¾ƒé«˜ |
| æœ€ä¼˜æ€§ | å¯èƒ½è¶…è¶Šä¸“å®¶ | å—é™äºä¸“å®¶ |
| å®‰å…¨æ€§ | é£é™©è¾ƒé«˜ | ç›¸å¯¹å®‰å…¨ |

### 1.3 åº”ç”¨åœºæ™¯

```
åº”ç”¨é¢†åŸŸï¼š
â”œâ”€â”€ æœºæ¢°è‡‚æ“ä½œ
â”‚   â”œâ”€ æŠ“å–ä¸æ”¾ç½®
â”‚   â”œâ”€ è£…é…ä»»åŠ¡
â”‚   â””â”€ ç²¾ç»†æ“ä½œï¼ˆç„Šæ¥ã€æ¶‚è£…ï¼‰
â”œâ”€â”€ ç§»åŠ¨æœºå™¨äºº
â”‚   â”œâ”€ å®¤å†…å¯¼èˆª
â”‚   â””â”€ è·¯å¾„è·Ÿéš
â”œâ”€â”€ è‡ªåŠ¨é©¾é©¶
â”‚   â”œâ”€ è½¦é“ä¿æŒ
â”‚   â””â”€ åœè½¦
â””â”€â”€ æœºå™¨äººå¯¼èˆª
    â””â”€ å¤æ‚ç¯å¢ƒç©¿è¶Š
```

---

## 2. è¡Œä¸ºå…‹éš†

### 2.1 åŸºæœ¬åŸç†

**Behavior Cloning (BC)** å°†æ¨¡ä»¿å­¦ä¹ è½¬åŒ–ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼š

$$\min_\theta \sum_{(s,a) \in D} L(\pi_\theta(s), a)$$

å…¶ä¸­ï¼š
- $D$ æ˜¯ä¸“å®¶ç¤ºèŒƒæ•°æ®é›†
- $L$ æ˜¯æŸå¤±å‡½æ•°ï¼ˆå¦‚MSEæˆ–äº¤å‰ç†µï¼‰

### 2.2 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è¡Œä¸ºå…‹éš†æ¶æ„                               â”‚
â”‚                                                             â”‚
â”‚   è¾“å…¥ï¼šçŠ¶æ€s                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚   â”‚  â”‚ å›¾åƒ/   â”‚   â”‚ ç‰¹å¾    â”‚   â”‚ MLP/    â”‚          â”‚   â”‚
â”‚   â”‚  â”‚ ä¼ æ„Ÿå™¨ â”‚â”€â”€â–¶â”‚ æå–å™¨  â”‚â”€â”€â–¶â”‚ Transformerâ”‚         â”‚   â”‚
â”‚   â”‚  â”‚ ç¼–ç å™¨  â”‚   â”‚         â”‚   â”‚         â”‚          â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚   â”‚                                   â”‚                â”‚   â”‚
â”‚   â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚   â”‚                    â–¼              â–¼              â–¼ â”‚   â”‚
â”‚   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚
â”‚   â”‚              â”‚ä½ç½®/é€Ÿåº¦ â”‚  â”‚ å¤¹çˆªæ§åˆ¶ â”‚  â”‚ å…¶ä»–åŠ¨ä½œ â”‚â”‚   â”‚
â”‚   â”‚              â”‚  è¾“å‡º    â”‚  â”‚  è¾“å‡º    â”‚  â”‚  è¾“å‡º    â”‚â”‚   â”‚
â”‚   â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚   æŸå¤±ï¼šL = MSE(Ï€(s), a_expert)                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Tuple, Dict
from torch.utils.data import Dataset, DataLoader

class BehaviorCloningPolicy(nn.Module):
    """è¡Œä¸ºå…‹éš†ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, 
                 state_dim: int, 
                 action_dim: int,
                 hidden_dims: List[int] = [256, 256],
                 use_visual: bool = False):
        super().__init__()
        
        self.use_visual = use_visual
        
        if use_visual:
            # è§†è§‰ç¼–ç å™¨
            self.visual_encoder = nn.Sequential(
                nn.Conv2d(3, 32, 8, stride=4),
                nn.ReLU(),
                nn.Conv2d(32, 64, 4, stride=2),
                nn.ReLU(),
                nn.Conv2d(64, 64, 3, stride=1),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(64 * 7 * 7, 512),
                nn.ReLU()
            )
            state_dim = 512 + state_dim
        
        # MLPç­–ç•¥ç½‘ç»œ
        layers = []
        prev_dim = state_dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim
        
        self.backbone = nn.Sequential(*layers)
        
        # åŠ¨ä½œè¾“å‡ºå¤´
        self.action_mean = nn.Linear(prev_dim, action_dim)
        self.action_log_std = nn.Parameter(torch.zeros(action_dim))
    
    def forward(self, state: torch.Tensor, image: torch.Tensor = None):
        if self.use_visual and image is not None:
            visual_features = self.visual_encoder(image)
            state = torch.cat([state, visual_features], dim=-1)
        
        features = self.backbone(state)
        action_mean = self.action_mean(features)
        action_std = torch.exp(self.action_log_std.clamp(-20, 2))
        
        return action_mean, action_std
    
    def get_action(self, state: np.ndarray, image: np.ndarray = None, 
                   deterministic: bool = False) -> np.ndarray:
        """è·å–åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            image_tensor = torch.FloatTensor(image).unsqueeze(0).permute(0, 3, 1, 2) if image is not None else None
            
            mean, std = self.forward(state_tensor, image_tensor)
            
            if deterministic:
                action = mean
            else:
                dist = torch.distributions.Normal(mean, std)
                action = dist.sample()
        
        return action.squeeze().numpy()


class DemonstrationDataset(Dataset):
    """ç¤ºèŒƒæ•°æ®é›†"""
    
    def __init__(self, demonstrations: List[Dict]):
        """
        demonstrations: ç¤ºèŒƒæ•°æ®åˆ—è¡¨
        æ¯ä¸ªå…ƒç´ æ˜¯ {'states': np.array, 'actions': np.array, 'images': np.array (å¯é€‰)}
        """
        self.states = []
        self.actions = []
        self.images = []
        
        for demo in demonstrations:
            self.states.extend(demo['states'])
            self.actions.extend(demo['actions'])
            if 'images' in demo:
                self.images.extend(demo['images'])
        
        self.states = np.array(self.states)
        self.actions = np.array(self.actions)
        self.has_images = len(self.images) > 0
        
        if self.has_images:
            self.images = np.array(self.images)
    
    def __len__(self):
        return len(self.states)
    
    def __getitem__(self, idx):
        state = self.states[idx]
        action = self.actions[idx]
        
        if self.has_images:
            image = self.images[idx]
            return state, action, image
        
        return state, action, None


class BehaviorCloningTrainer:
    """è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 policy: BehaviorCloningPolicy,
                 lr: float = 1e-4,
                 weight_decay: float = 1e-5,
                 device: str = 'cuda'):
        
        self.policy = policy.to(device)
        self.device = device
        self.optimizer = optim.AdamW(
            policy.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000, eta_min=1e-6
        )
    
    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.policy.train()
        total_loss = 0
        total_mse = 0
        num_batches = 0
        
        for batch in dataloader:
            if len(batch) == 3:
                states, actions, images = batch
                images = images.to(self.device) if images[0] is not None else None
            else:
                states, actions = batch
                images = None
            
            states = states.to(self.device)
            actions = actions.to(self.device)
            
            # å‰å‘ä¼ æ’­
            pred_mean, pred_std = self.policy(states, images)
            
            # è®¡ç®—æŸå¤±
            # MSEæŸå¤±
            mse_loss = nn.MSELoss()(pred_mean, actions)
            
            # è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆå¯é€‰ï¼‰
            dist = torch.distributions.Normal(pred_mean, pred_std)
            nll_loss = -dist.log_prob(actions).mean()
            
            # æ€»æŸå¤±
            loss = mse_loss + 0.1 * nll_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            total_mse += mse_loss.item()
            num_batches += 1
        
        self.scheduler.step()
        
        return {
            'loss': total_loss / num_batches,
            'mse': total_mse / num_batches,
            'lr': self.scheduler.get_last_lr()[0]
        }
    
    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.policy.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in dataloader:
                if len(batch) == 3:
                    states, actions, images = batch
                    images = images.to(self.device) if images[0] is not None else None
                else:
                    states, actions = batch
                    images = None
                
                states = states.to(self.device)
                actions = actions.to(self.device)
                
                pred_mean, _ = self.policy(states, images)
                loss = nn.MSELoss()(pred_mean, actions)
                
                total_loss += loss.item()
                num_batches += 1
        
        return {'eval_loss': total_loss / num_batches}


def train_behavior_cloning(demonstrations: List[Dict],
                          state_dim: int,
                          action_dim: int,
                          num_epochs: int = 100,
                          batch_size: int = 64,
                          use_visual: bool = False):
    """è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹"""
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = DemonstrationDataset(demonstrations)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    
    # åˆ›å»ºæ¨¡å‹
    policy = BehaviorCloningPolicy(state_dim, action_dim, use_visual=use_visual)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BehaviorCloningTrainer(policy)
    
    # è®­ç»ƒ
    for epoch in range(num_epochs):
        metrics = trainer.train_epoch(dataloader)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}")
            print(f"  Loss: {metrics['loss']:.4f}, MSE: {metrics['mse']:.4f}")
    
    return policy
```

### 2.4 BCçš„é—®é¢˜

**åˆ†å¸ƒåç§»ï¼ˆDistribution Shiftï¼‰**ï¼š

```
é—®é¢˜ï¼šè®­ç»ƒæ—¶çŠ¶æ€åˆ†å¸ƒ â‰  æµ‹è¯•æ—¶çŠ¶æ€åˆ†å¸ƒ

è®­ç»ƒï¼šs ~ D_expert (ä¸“å®¶çŠ¶æ€åˆ†å¸ƒ)
æµ‹è¯•ï¼šs ~ D_Ï€ (ç­–ç•¥çŠ¶æ€åˆ†å¸ƒ)

å¦‚æœÏ€çŠ¯é”™ â†’ åˆ°è¾¾æ–°çŠ¶æ€ â†’ ç»§ç»­çŠ¯é”™ â†’ ç´¯ç§¯è¯¯å·®

        ä¸“å®¶è½¨è¿¹                    å­¦ä¹ ç­–ç•¥è½¨è¿¹
           â”‚                           â”‚
           â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  sâ‚ â†’ sâ‚‚ â†’ sâ‚ƒ â”‚            â”‚  sâ‚ â†’ sâ‚‚'â†’ ???â”‚
    â”‚    â†˜   â†™     â”‚            â”‚      â†˜       â”‚
    â”‚     æˆåŠŸ      â”‚            â”‚      å¤±è´¥    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. DAggerç®—æ³•

### 3.1 æ ¸å¿ƒæ€æƒ³

**Dataset Aggregation (DAgger)** é€šè¿‡è¿­ä»£æ”¶é›†æ•°æ®æ¥è§£å†³åˆ†å¸ƒåç§»é—®é¢˜ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DAggeræµç¨‹                              â”‚
â”‚                                                             â”‚
â”‚   åˆå§‹åŒ–ï¼šD = ä¸“å®¶æ•°æ®                                       â”‚
â”‚                                                             â”‚
â”‚   for i = 1 to N:                                          â”‚
â”‚       1. è®­ç»ƒç­–ç•¥ Ï€áµ¢ åœ¨æ•°æ®é›†Dä¸Š                            â”‚
â”‚       2. æ‰§è¡ŒÏ€áµ¢ï¼Œæ”¶é›†çŠ¶æ€åºåˆ— sâ‚, sâ‚‚, ..., sâ‚œ              â”‚
â”‚       3. è¯·ä¸“å®¶æ ‡æ³¨åŠ¨ä½œ aâ‚, aâ‚‚, ..., aâ‚œ                    â”‚
â”‚       4. å°†æ–°æ•°æ®åŠ å…¥D                                      â”‚
â”‚       5. D = D âˆª {(sâ±¼, aâ±¼)}                                â”‚
â”‚                                                             â”‚
â”‚   è¿”å›æœ€ç»ˆç­–ç•¥ Ï€â‚™                                           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ä»£ç å®ç°

```python
from typing import Callable, Tuple
import copy

class DAggerTrainer:
    """DAggerè®­ç»ƒå™¨"""
    
    def __init__(self,
                 policy: BehaviorCloningPolicy,
                 expert_policy: Callable,
                 env,
                 beta_schedule: Callable = None,
                 lr: float = 1e-4,
                 device: str = 'cuda'):
        
        self.policy = policy.to(device)
        self.expert_policy = expert_policy
        self.env = env
        self.device = device
        
        # Î²è°ƒåº¦ï¼šæ··åˆä¸“å®¶å’Œå­¦ä¹ çš„ç­–ç•¥
        if beta_schedule is None:
            # é»˜è®¤ï¼šçº¿æ€§è¡°å‡
            self.beta_schedule = lambda epoch: max(0.0, 1.0 - epoch / 20)
        else:
            self.beta_schedule = beta_schedule
        
        self.bc_trainer = BehaviorCloningTrainer(policy, lr=lr)
        
        # å­˜å‚¨æ‰€æœ‰æ•°æ®
        self.all_demonstrations = []
    
    def collect_data_with_policy(self, 
                                  num_episodes: int = 10,
                                  beta: float = 0.0) -> List[Dict]:
        """ä½¿ç”¨å½“å‰ç­–ç•¥æ”¶é›†æ•°æ®ï¼Œå¹¶ç”¨ä¸“å®¶æ ‡æ³¨"""
        demonstrations = []
        
        for ep in range(num_episodes):
            states = []
            expert_actions = []
            
            state, _ = self.env.reset()
            done = False
            
            while not done:
                # è·å–ä¸“å®¶åŠ¨ä½œ
                expert_action = self.expert_policy(state)
                
                # è·å–å­¦ä¹ ç­–ç•¥åŠ¨ä½œ
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                    learned_action, _ = self.policy(state_tensor)
                    learned_action = learned_action.squeeze().cpu().numpy()
                
                # æ··åˆåŠ¨ä½œ
                action = beta * expert_action + (1 - beta) * learned_action
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # å­˜å‚¨çŠ¶æ€å’Œä¸“å®¶æ ‡æ³¨
                states.append(state)
                expert_actions.append(expert_action)
                
                state = next_state
            
            demonstrations.append({
                'states': states,
                'actions': expert_actions
            })
        
        return demonstrations
    
    def train(self,
              initial_demos: List[Dict],
              num_iterations: int = 20,
              episodes_per_iter: int = 10,
              bc_epochs_per_iter: int = 50,
              batch_size: int = 64) -> BehaviorCloningPolicy:
        """DAggerè®­ç»ƒ"""
        
        # åˆå§‹åŒ–æ•°æ®é›†
        self.all_demonstrations = copy.deepcopy(initial_demos)
        
        best_policy = None
        best_loss = float('inf')
        
        for iteration in range(num_iterations):
            beta = self.beta_schedule(iteration)
            
            print(f"\n=== Iteration {iteration + 1}/{num_iterations} ===")
            print(f"Beta (expert mixing): {beta:.2f}")
            
            # 1. æ”¶é›†æ–°æ•°æ®
            print("Collecting data...")
            new_demos = self.collect_data_with_policy(
                num_episodes=episodes_per_iter,
                beta=beta
            )
            
            # 2. åŠ å…¥æ•°æ®é›†
            self.all_demonstrations.extend(new_demos)
            print(f"Total demonstrations: {len(self.all_demonstrations)}")
            
            # 3. è®­ç»ƒ
            print("Training policy...")
            dataset = DemonstrationDataset(self.all_demonstrations)
            dataloader = DataLoader(
                dataset, 
                batch_size=batch_size,
                shuffle=True,
                num_workers=4
            )
            
            for epoch in range(bc_epochs_per_iter):
                metrics = self.bc_trainer.train_epoch(dataloader)
                
                if (epoch + 1) % 10 == 0:
                    print(f"  Epoch {epoch+1}: Loss={metrics['loss']:.4f}")
            
            # 4. è¯„ä¼°å¹¶ä¿å­˜æœ€ä½³ç­–ç•¥
            eval_metrics = self.bc_trainer.evaluate(dataloader)
            if eval_metrics['eval_loss'] < best_loss:
                best_loss = eval_metrics['eval_loss']
                best_policy = copy.deepcopy(self.policy)
                print(f"  New best loss: {best_loss:.4f}")
        
        return best_policy


# ç¤ºä¾‹ï¼šä¸“å®¶ç­–ç•¥
class ExpertPolicy:
    """ç¤ºä¾‹ä¸“å®¶ç­–ç•¥ï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®ä¸“å®¶ï¼‰"""
    
    def __init__(self, env):
        self.env = env
    
    def __call__(self, state: np.ndarray) -> np.ndarray:
        """
        æ ¹æ®çŠ¶æ€è¿”å›ä¸“å®¶åŠ¨ä½œ
        å®é™…åº”ç”¨ä¸­å¯èƒ½æ˜¯ï¼š
        - é¢„è®­ç»ƒçš„é«˜æ€§èƒ½ç­–ç•¥
        - äººç±»é¥æ“ä½œ
        - è§„åˆ’ç®—æ³•
        """
        # ç®€å•ç¤ºä¾‹ï¼šPDæ§åˆ¶å™¨
        target_pos = self.env.target_pos
        current_pos = state[:3]
        
        action = (target_pos - current_pos) * 2.0  # ç®€å•æ¯”ä¾‹æ§åˆ¶
        action = np.clip(action, -1, 1)
        
        return action
```

---

## 4. GAIL

### 4.1 æ ¸å¿ƒæ€æƒ³

**Generative Adversarial Imitation Learning** ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æ€æƒ³è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GAILæ¶æ„                               â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚   ä¸“å®¶æ•°æ®    â”‚                    â”‚   ç”Ÿæˆå™¨G     â”‚      â”‚
â”‚   â”‚   Ï€_E        â”‚                    â”‚  (ç­–ç•¥Ï€_Î¸)   â”‚      â”‚
â”‚   â”‚              â”‚                    â”‚              â”‚      â”‚
â”‚   â”‚ Ï„_E ~ Ï€_E    â”‚                    â”‚ Ï„ ~ Ï€_Î¸      â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚           â”‚                                   â”‚             â”‚
â”‚           â”‚     çŠ¶æ€-åŠ¨ä½œå¯¹ (s,a)             â”‚             â”‚
â”‚           â”‚                                   â”‚             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                 â”‚   åˆ¤åˆ«å™¨D     â”‚                            â”‚
â”‚                 â”‚ D(s,a) â†’ [0,1]â”‚                           â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚              D(s,a) â‰ˆ 1: æ¥è‡ªä¸“å®¶                          â”‚
â”‚              D(s,a) â‰ˆ 0: æ¥è‡ªç”Ÿæˆå™¨                         â”‚
â”‚                                                             â”‚
â”‚   è®­ç»ƒç›®æ ‡ï¼š                                                 â”‚
â”‚   - åˆ¤åˆ«å™¨ï¼šåŒºåˆ†ä¸“å®¶å’Œç”Ÿæˆå™¨è½¨è¿¹                             â”‚
â”‚   - ç”Ÿæˆå™¨ï¼šæ¬ºéª—åˆ¤åˆ«å™¨ï¼ˆç”Ÿæˆç±»ä¼¼ä¸“å®¶çš„è½¨è¿¹ï¼‰                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ•°å­¦å½¢å¼

**åˆ¤åˆ«å™¨ç›®æ ‡**ï¼š
$$\max_D \mathbb{E}_{(s,a)\sim\pi_E}[\log D(s,a)] + \mathbb{E}_{(s,a)\sim\pi_\theta}[\log(1-D(s,a))]$$

**ç”Ÿæˆå™¨ï¼ˆç­–ç•¥ï¼‰ç›®æ ‡**ï¼š
$$\min_\theta \mathbb{E}_{(s,a)\sim\pi_\theta}[\log(1-D(s,a))] - \lambda H(\pi_\theta)$$

å…¶ä¸­ $H(\pi_\theta)$ æ˜¯ç­–ç•¥ç†µï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰ã€‚

### 4.3 ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Dict

class Discriminator(nn.Module):
    """GAILåˆ¤åˆ«å™¨"""
    
    def __init__(self, 
                 state_dim: int, 
                 action_dim: int,
                 hidden_dim: int = 256):
        super().__init__()
        
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """è¾“å‡ºæ¦‚ç‡ [0, 1]"""
        x = torch.cat([state, action], dim=-1)
        return self.net(x)


class GAILAgent:
    """GAILæ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 expert_data: List[Dict],
                 lr_policy: float = 3e-4,
                 lr_disc: float = 1e-4,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 entropy_coef: float = 0.01,
                 device: str = 'cuda'):
        
        self.device = device
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.entropy_coef = entropy_coef
        
        # ç­–ç•¥ç½‘ç»œ
        self.policy = PolicyNetwork(state_dim, action_dim).to(device)
        self.critic = CriticNetwork(state_dim).to(device)
        
        # åˆ¤åˆ«å™¨
        self.discriminator = Discriminator(state_dim, action_dim).to(device)
        
        # ä¼˜åŒ–å™¨
        self.policy_optimizer = optim.Adam(
            list(self.policy.parameters()) + list(self.critic.parameters()),
            lr=lr_policy
        )
        self.disc_optimizer = optim.Adam(
            self.discriminator.parameters(),
            lr=lr_disc
        )
        
        # é¢„å¤„ç†ä¸“å®¶æ•°æ®
        self.expert_states = torch.FloatTensor(
            np.concatenate([d['states'] for d in expert_data])
        ).to(device)
        self.expert_actions = torch.FloatTensor(
            np.concatenate([d['actions'] for d in expert_data])
        ).to(device)
        
        # ç»éªŒç¼“å†²åŒº
        self.replay_buffer = []
    
    def collect_trajectories(self, env, num_steps: int = 2048):
        """æ”¶é›†è½¨è¿¹"""
        self.policy.eval()
        
        states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []
        
        state, _ = env.reset()
        
        for _ in range(num_steps):
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                mean, std = self.policy(state_tensor)
                dist = torch.distributions.Normal(mean, std)
                action = dist.sample()
                log_prob = dist.log_prob(action).sum(dim=-1)
                value = self.critic(state_tensor)
            
            action_np = action.squeeze().cpu().numpy()
            next_state, _, terminated, truncated, _ = env.step(action_np)
            done = terminated or truncated
            
            states.append(state)
            actions.append(action_np)
            dones.append(done)
            log_probs.append(log_prob.item())
            values.append(value.item())
            
            state = next_state if not done else env.reset()[0]
        
        return {
            'states': np.array(states),
            'actions': np.array(actions),
            'dones': np.array(dones),
            'log_probs': np.array(log_probs),
            'values': np.array(values)
        }
    
    def train_discriminator(self, agent_data: Dict, batch_size: int = 256) -> float:
        """è®­ç»ƒåˆ¤åˆ«å™¨"""
        self.discriminator.train()
        
        agent_states = torch.FloatTensor(agent_data['states']).to(self.device)
        agent_actions = torch.FloatTensor(agent_data['actions']).to(self.device)
        
        num_samples = len(agent_states)
        indices = np.random.permutation(num_samples)
        
        total_loss = 0
        num_batches = 0
        
        for start in range(0, num_samples, batch_size):
            end = start + batch_size
            idx = indices[start:end]
            
            # Agentæ•°æ®
            agent_s = agent_states[idx]
            agent_a = agent_actions[idx]
            
            # éšæœºé‡‡æ ·ä¸“å®¶æ•°æ®
            expert_idx = np.random.randint(0, len(self.expert_states), len(idx))
            expert_s = self.expert_states[expert_idx]
            expert_a = self.expert_actions[expert_idx]
            
            # åˆ¤åˆ«å™¨é¢„æµ‹
            expert_pred = self.discriminator(expert_s, expert_a)
            agent_pred = self.discriminator(agent_s, agent_a)
            
            # æŸå¤±
            expert_loss = -torch.log(expert_pred + 1e-8).mean()
            agent_loss = -torch.log(1 - agent_pred + 1e-8).mean()
            loss = expert_loss + agent_loss
            
            self.disc_optimizer.zero_grad()
            loss.backward()
            self.disc_optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        return total_loss / num_batches
    
    def compute_gail_reward(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—GAILå¥–åŠ±"""
        with torch.no_grad():
            d = self.discriminator(states, actions)
            # å¥–åŠ± = log(D) - log(1-D) = logit(D)
            # æˆ–è€…ç®€åŒ–ä¸º -log(1-D) æ¥é¼“åŠ±æ¬ºéª—åˆ¤åˆ«å™¨
            reward = -torch.log(1 - d + 1e-8)
        return reward.squeeze()
    
    def train_policy(self, agent_data: Dict, epochs: int = 10, batch_size: int = 64) -> float:
        """è®­ç»ƒç­–ç•¥ï¼ˆä½¿ç”¨PPOï¼‰"""
        self.policy.train()
        self.critic.train()
        
        states = torch.FloatTensor(agent_data['states']).to(self.device)
        actions = torch.FloatTensor(agent_data['actions']).to(self.device)
        old_log_probs = torch.FloatTensor(agent_data['log_probs']).to(self.device)
        values = agent_data['values']
        dones = agent_data['dones']
        
        # è®¡ç®—GAILå¥–åŠ±
        gail_rewards = self.compute_gail_reward(states, actions)
        
        # è®¡ç®—GAE
        advantages = []
        gae = 0
        for t in reversed(range(len(gail_rewards))):
            if t == len(gail_rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = gail_rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        advantages = torch.FloatTensor(advantages).to(self.device)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        returns = advantages + torch.FloatTensor(values).to(self.device)
        
        total_loss = 0
        
        for _ in range(epochs):
            indices = np.random.permutation(len(states))
            
            for start in range(0, len(states), batch_size):
                idx = indices[start:start + batch_size]
                
                batch_states = states[idx]
                batch_actions = actions[idx]
                batch_old_log_probs = old_log_probs[idx]
                batch_advantages = advantages[idx]
                batch_returns = returns[idx]
                
                # è®¡ç®—æ–°log_prob
                mean, std = self.policy(batch_states)
                dist = torch.distributions.Normal(mean, std)
                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)
                entropy = dist.entropy().sum(dim=-1).mean()
                
                # PPOæŸå¤±
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 0.8, 1.2) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # ä»·å€¼æŸå¤±
                value_pred = self.critic(batch_states).squeeze()
                value_loss = nn.MSELoss()(value_pred, batch_returns)
                
                # æ€»æŸå¤±
                loss = policy_loss + 0.5 * value_loss - self.entropy_coef * entropy
                
                self.policy_optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(self.policy.parameters()) + list(self.critic.parameters()),
                    0.5
                )
                self.policy_optimizer.step()
                
                total_loss += loss.item()
        
        return total_loss / (epochs * (len(states) // batch_size + 1))
    
    def train(self, env, num_iterations: int = 100, steps_per_iter: int = 2048):
        """å®Œæ•´è®­ç»ƒ"""
        for iteration in range(num_iterations):
            # æ”¶é›†è½¨è¿¹
            agent_data = self.collect_trajectories(env, steps_per_iter)
            
            # è®­ç»ƒåˆ¤åˆ«å™¨
            disc_loss = self.train_discriminator(agent_data)
            
            # è®­ç»ƒç­–ç•¥
            policy_loss = self.train_policy(agent_data)
            
            if (iteration + 1) % 10 == 0:
                print(f"Iteration {iteration + 1}")
                print(f"  Disc Loss: {disc_loss:.4f}")
                print(f"  Policy Loss: {policy_loss:.4f}")
```

---

## 5. ä»£ç å®ç°

### å®Œæ•´ç¤ºä¾‹ï¼šæœºæ¢°è‡‚æŠ“å–

```python
import gymnasium as gym
import numpy as np
from tqdm import tqdm

# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæœºæ¢°è‡‚ç¯å¢ƒ
class RobotArmEnv:
    """ç®€åŒ–çš„æœºæ¢°è‡‚ç¯å¢ƒ"""
    
    def __init__(self):
        self.state_dim = 10  # å…³èŠ‚è§’åº¦ + ç›®æ ‡ä½ç½®
        self.action_dim = 7  # 7ä¸ªå…³èŠ‚
        
        self.reset()
    
    def reset(self):
        self.joint_pos = np.zeros(7)
        self.target_pos = np.random.uniform(-0.5, 0.5, 3)
        return self._get_state()
    
    def _get_state(self):
        # ç®€åŒ–çš„æ­£å‘è¿åŠ¨å­¦
        ee_pos = self.joint_pos[:3]  # ç®€åŒ–
        return np.concatenate([self.joint_pos, ee_pos, self.target_pos])
    
    def step(self, action):
        self.joint_pos = np.clip(self.joint_pos + action * 0.1, -1, 1)
        
        ee_pos = self.joint_pos[:3]
        distance = np.linalg.norm(ee_pos - self.target_pos)
        
        reward = -distance
        done = distance < 0.05
        
        if done:
            reward += 10
        
        return self._get_state(), reward, done, {}


def collect_expert_demonstrations(env, num_demos: int = 50, demo_length: int = 100):
    """æ”¶é›†ä¸“å®¶ç¤ºèŒƒï¼ˆå®é™…ä¸­ä½¿ç”¨é¥æ“ä½œæˆ–è§„åˆ’ç®—æ³•ï¼‰"""
    demonstrations = []
    
    for _ in range(num_demos):
        states = []
        actions = []
        
        state = env.reset()
        
        for t in range(demo_length):
            # ç®€å•ä¸“å®¶ï¼šå‘ç›®æ ‡ç§»åŠ¨
            target = state[-3:]
            current = state[7:10]
            action = (target - current) * 0.5  # ç®€å•PDæ§åˆ¶
            action = np.clip(action, -1, 1)
            action = np.concatenate([action, np.zeros(4)])  # 7ç»´åŠ¨ä½œ
            
            states.append(state)
            actions.append(action)
            
            state, _, done, _ = env.step(action)
            
            if done:
                break
        
        demonstrations.append({
            'states': np.array(states),
            'actions': np.array(actions)
        })
    
    return demonstrations


def run_behavior_cloning():
    """è¿è¡Œè¡Œä¸ºå…‹éš†"""
    env = RobotArmEnv()
    
    # æ”¶é›†ä¸“å®¶ç¤ºèŒƒ
    print("Collecting expert demonstrations...")
    demos = collect_expert_demonstrations(env, num_demos=100)
    
    # è®­ç»ƒBC
    print("Training behavior cloning...")
    policy = train_behavior_cloning(
        demos,
        state_dim=env.state_dim,
        action_dim=env.action_dim,
        num_epochs=200
    )
    
    # æµ‹è¯•
    print("Testing policy...")
    test_rewards = []
    for _ in range(10):
        state = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            action = policy.get_action(state, deterministic=True)
            state, reward, done, _ = env.step(action)
            total_reward += reward
        
        test_rewards.append(total_reward)
    
    print(f"Average test reward: {np.mean(test_rewards):.2f}")


def run_dagger():
    """è¿è¡ŒDAgger"""
    env = RobotArmEnv()
    
    # åˆå§‹ä¸“å®¶æ•°æ®
    print("Collecting initial demonstrations...")
    initial_demos = collect_expert_demonstrations(env, num_demos=20)
    
    # åˆ›å»ºä¸“å®¶ç­–ç•¥
    def expert_policy(state):
        target = state[-3:]
        current = state[7:10]
        action = (target - current) * 0.5
        return np.concatenate([np.clip(action, -1, 1), np.zeros(4)])
    
    # åˆ›å»ºDAggerè®­ç»ƒå™¨
    policy = BehaviorCloningPolicy(env.state_dim, env.action_dim)
    trainer = DAggerTrainer(policy, expert_policy, env)
    
    # è®­ç»ƒ
    print("Training with DAgger...")
    final_policy = trainer.train(
        initial_demos,
        num_iterations=30,
        episodes_per_iter=5,
        bc_epochs_per_iter=50
    )
    
    return final_policy


if __name__ == "__main__":
    print("=== Behavior Cloning ===")
    run_behavior_cloning()
    
    print("\n=== DAgger ===")
    run_dagger()
```

---

## 6. å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šLeRobotæ¡†æ¶ä½¿ç”¨

```python
# ä½¿ç”¨HuggingFace LeRobotè¿›è¡Œæ¨¡ä»¿å­¦ä¹ 
from lerobot.common.policies.act.modeling_act import ACTPolicy
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset

# åŠ è½½æ•°æ®é›†
dataset = LeRobotDataset("pusht")

# åˆ›å»ºACTç­–ç•¥
policy = ACTPolicy(
    input_shapes=dataset.meta.observation_shapes,
    output_shapes=dataset.meta.action_shapes,
    chunk_size=100,
    n_obs_steps=2,
)

# è®­ç»ƒ
from lerobot.common.train import train_policy

train_policy(
    policy=policy,
    dataset=dataset,
    num_epochs=100,
    batch_size=8,
    learning_rate=1e-4,
)
```

### æ¡ˆä¾‹2ï¼šDiffusion Policy

```python
# Diffusion Policyå®ç°
class DiffusionPolicy(nn.Module):
    """æ‰©æ•£ç­–ç•¥"""
    
    def __init__(self, 
                 state_dim: int,
                 action_dim: int,
                 horizon: int = 16,
                 num_diffusion_steps: int = 100):
        super().__init__()
        
        self.horizon = horizon
        self.action_dim = action_dim
        self.num_diffusion_steps = num_diffusion_steps
        
        # å™ªå£°é¢„æµ‹ç½‘ç»œ
        self.noise_net = nn.Sequential(
            nn.Linear(state_dim + action_dim * horizon + 1, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, action_dim * horizon)
        )
        
        # Î²è°ƒåº¦
        self.betas = torch.linspace(1e-4, 0.02, num_diffusion_steps)
        self.alphas = 1 - self.betas
        self.alpha_bars = torch.cumprod(self.alphas, dim=0)
    
    def forward(self, state: torch.Tensor, noisy_actions: torch.Tensor, t: torch.Tensor):
        """é¢„æµ‹å™ªå£°"""
        batch_size = state.shape[0]
        
        # å±•å¹³åŠ¨ä½œ
        noisy_actions = noisy_actions.view(batch_size, -1)
        
        # æ—¶é—´ç¼–ç 
        t_emb = t.float().unsqueeze(-1) / self.num_diffusion_steps
        
        # æ‹¼æ¥è¾“å…¥
        x = torch.cat([state, noisy_actions, t_emb], dim=-1)
        
        # é¢„æµ‹å™ªå£°
        noise_pred = self.noise_net(x)
        
        return noise_pred.view(batch_size, self.horizon, self.action_dim)
    
    def sample(self, state: torch.Tensor) -> torch.Tensor:
        """é‡‡æ ·åŠ¨ä½œåºåˆ—"""
        batch_size = state.shape[0]
        
        # ä»çº¯å™ªå£°å¼€å§‹
        actions = torch.randn(batch_size, self.horizon, self.action_dim)
        
        # é€æ­¥å»å™ª
        for t in reversed(range(self.num_diffusion_steps)):
            t_tensor = torch.full((batch_size,), t, dtype=torch.long)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = self.forward(state, actions, t_tensor)
            
            # å»å™ªæ­¥éª¤
            alpha = self.alphas[t]
            alpha_bar = self.alpha_bars[t]
            
            if t > 0:
                noise = torch.randn_like(actions)
                sigma = self.betas[t] ** 0.5
            else:
                noise = 0
                sigma = 0
            
            actions = (1 / alpha ** 0.5) * (
                actions - (1 - alpha) / (1 - alpha_bar) ** 0.5 * noise_pred
            ) + sigma * noise
        
        return actions
```

---

## ğŸ“š æ¨èèµ„æº

### ç»å…¸è®ºæ–‡
- **BC**: A Reduction of Imitation Learning to RL (Ross et al., 2011)
- **DAgger**: A Reduction of IL to RL (Ross et al., 2011)
- **GAIL**: Generative Adversarial Imitation Learning (Ho & Ermon, 2016)
- **ACT**: Learning Fine-Grained Bimanual Manipulation (Zhao et al., 2023)
- **Diffusion Policy**: Diffusion Policy Visuomotor Policy Learning (Chi et al., 2023)

### å¼€æºé¡¹ç›®
- [LeRobot](https://github.com/huggingface/lerobot) - HuggingFaceæœºå™¨äººå­¦ä¹ æ¡†æ¶
- [iLQR](https://github.com/anassinator/ilqr) - è¿­ä»£LQR
- [mjrl](https://github.com/aravindr93/mjrl) - MuJoCo RLåº“

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼*
