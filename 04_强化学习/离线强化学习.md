# 离线强化学习（Offline RL）

离线强化学习（Offline Reinforcement Learning）又称批强化学习（Batch RL），是指从固定的数据集中学习策略，不需要在线与环境交互。这对机器人学习具有重要意义，因为实机器人在线交互成本高且存在安全风险。

## 目录

- [1. 离线强化学习概述](#1-离线强化学习概述)
- [2. 分布偏移问题](#2-分布偏移问题)
- [3. 基于约束的方法](#3-基于约束的方法)
- [4. 基于模型的方法](#4-基于模型的方法)
- [5. 经验回放方法](#5-经验回放方法)
- [6. 实践框架](#6-实践框架)

---

## 1. 离线强化学习概述

### 1.1 离线RL vs 在线RL

| 特点 | 在线RL | 离线RL |
|------|--------|--------|
| 数据收集 | 在线与环境交互 | 预先收集的固定数据集 |
| 探索 | 需要探索 | 无需探索 |
| 样本效率 | 较低 | 较高 |
| 安全风险 | 可能有风险 | 无风险 |
| 分布偏移 | 无 | 存在 |

### 1.2 离线RL的挑战

```
核心问题：分布偏移（Distribution Shift）

在线RL: 策略收集数据 → 学习策略 → 收集新数据 → ...
离线RL: 固定数据集 → 学习策略 (分布偏移!)

策略采集的数据分布 ≠ 数据集中的分布
```

---

## 2. 分布偏移问题

### 2.1 问题的本质

```
数据集: D = {(s, a, r, s')}
策略: π(a|s)

学到的Q(s, a) 是基于数据集中的 (s, a) 对
但部署时 π(a|s) 可能产生数据集中未出现过的动作
```

### 2.2 误差传播

```python
# 经典DQN在离线设置中的问题
class OfflineRLProblem:
    def demonstrate_extrapolation_error():
        """
        外推误差示例
        
        假设数据集只包含小范围的动作
        当策略选择稍大范围的动作时，Q值估计会严重偏差
        """
        pass
    
    def demonstrate_overestimation():
        """
        过估计问题
        
        最大化Q值时倾向于选择数据集中未覆盖的动作
        导致策略退化
        """
        pass
```

---

## 3. 基于约束的方法

### 3.1 Constrained Policy Optimization (CPO)

```python
import torch
import torch.nn as nn
import numpy as np

class ConservativeQLearning:
    """
    Conservative Q-Learning (CQL)
    核心思想：通过惩罚未覆盖动作的Q值来避免分布偏移
    """
    def __init__(self, state_dim, action_dim, hidden_dim=256, gamma=0.99):
        self.gamma = gamma
        
        # Q网络
        self.Q1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.Q2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.Q1_target = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        self.Q1_target.load_state_dict(self.Q1.state_dict())
        
        self.Q2_target = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        self.Q2_target.load_state_dict(self.Q2.state_dict())
        
        self.policy = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        
        self.actor_optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)
        self.critic_optimizer = torch.optim.Adam(
            list(self.Q1.parameters()) + list(self.Q2.parameters()), lr=3e-4
        )
        
    def update(self, batch, alpha=1.0):
        states, actions, rewards, next_states, dones = batch
        
        # ===== Critic Update (CQL) =====
        # 标准TD损失
        with torch.no_grad():
            next_actions = self.policy(next_states)
            next_q1 = self.Q1_target(torch.cat([next_states, next_actions], dim=-1))
            next_q2 = self.Q2_target(torch.cat([next_states, next_actions], dim=-1))
            next_q = torch.min(next_q1, next_q2)
            target_q = rewards + (1 - dones) * self.gamma * next_q
        
        # 当前Q值
        current_q1 = self.Q1(torch.cat([states, actions], dim=-1)).squeeze()
        current_q2 = self.Q2(torch.cat([states, actions], dim=-1)).squeeze()
        
        # TD损失
        critic_loss = nn.MSELoss()(current_q1, target_q) + \
                      nn.MSELoss()(current_q2, target_q)
        
        # CQL正则化损失
        # 采样动作并计算其Q值
        random_actions = torch.rand_like(actions) * 2 - 1  # 均匀分布
        sampled_actions = self.policy(states).detach()
        
        # 计算未覆盖动作的Q值
        q_random = self.Q1(torch.cat([states, random_actions], dim=-1)).mean()
        q_sampled = self.Q1(torch.cat([states, sampled_actions], dim=-1)).mean()
        
        # CQL损失：鼓励Q值下界
        cql_loss = alpha * (q_random - q_sampled)
        
        total_critic_loss = critic_loss - cql_loss
        
        self.critic_optimizer.zero_grad()
        total_critic_loss.backward()
        self.critic_optimizer.step()
        
        # ===== Actor Update =====
        new_actions = self.policy(states)
        q_new = self.Q1(torch.cat([states, new_actions], dim=-1)).mean()
        
        actor_loss = -q_new
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        return {
            'critic_loss': critic_loss.item(),
            'cql_loss': cql_loss.item(),
            'actor_loss': actor_loss.item()
        }
```

### 3.2 Policy Constraint方法

```python
class BehavioralCloningPolicy:
    """
    行为克隆：学习模仿数据集的策略
    作为约束项加入RL目标
    """
    def __init__(self, state_dim, action_dim):
        self.policy = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()
        )
        
    def behavior_clone(self, states, expert_actions):
        """行为克隆损失"""
        pred_actions = self.policy(states)
        loss = nn.MSELoss()(pred_actions, expert_actions)
        return loss


class BCQAgent:
    """
    BCQ (Batch Constrained Q-Learning)
    核心：在约束下进行最大化
    """
    def __init__(self, state_dim, action_dim):
        # 变分自编码器生成类似专家的动作
        self.vae = VAE(state_dim, action_dim)
        
        # Q网络
        self.Q = QNetwork(state_dim, action_dim)
        self.Q_target = QNetwork(state_dim, action_dim)
        
        # 扰动网络
        self.perturb = PerturbationNetwork(state_dim, action_dim)
        
    def select_action(self, state):
        # 生成动作
        recon_action, _, _ = self.vae(state)
        
        # 添加扰动
        perturbation = self.perturb(state, recon_action)
        action = recon_action + perturbation
        
        return torch.clamp(action, -1, 1)
```

---

## 4. 基于模型的方法

### 4.1 模型预测控制 (MPC)

```python
class ModelBasedRL:
    """
    基于模型的RL：先学习环境模型，再用模型进行规划
    """
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # 学习环境动力学模型
        self.dynamics_model = DynamicsModel(state_dim, action_dim)
        
        # 控制器
        self.controller = MPCController(self.dynamics_model)
        
    def update_model(self, dataset):
        """使用数据学习动力学模型"""
        # 监督学习: s' = f(s, a)
        states, actions, next_states = dataset['s'], dataset['a'], dataset['s']
        
        loss = self.dynamics_model.fit(states, actions, next_states)
        return loss
    
    def plan(self, state, horizon=10):
        """使用模型进行规划"""
        return self.controller.solve(state, horizon)
```

### 4.2 Dreamer架构

```python
class Dreamer:
    """
    Dreamer: 从图像学习世界模型
    核心：想象 rollout
    """
    def __init__(self, state_dim, action_dim):
        # 世界模型
        self.encoder = Encoder()
        self.dynamics = RSSM()  # 循环状态空间模型
        self.reward_model = RewardModel()
        self.decoder = Decoder()
        
        # 策略和价值网络
        self.actor = Actor()
        self.critic = Critic()
        
    def imagine_rollout(self, latent, policy, horizon=50):
        """想象 rollout"""
        trajectory = [latent]
        
        for _ in range(horizon):
            action = policy(latent)
            latent, reward = self.dynamics.step(latent, action)
            trajectory.append(latent)
            
        return trajectory
```

---

## 5. 经验回放方法

### 5.1 带权重的经验回放

```python
class WeightedReplayBuffer:
    """
    优先经验回放 (Prioritized Experience Replay)
    优先回放高TD误差的经验
    """
    def __init__(self, capacity=100000, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha  # 优先级指数
        self.beta = beta     # 重要性采样指数
        
        self.buffer = []
        self.priorities = []
        
    def push(self, state, action, reward, next_state, done):
        # 最高优先级
        max_priority = max(self.priorities) if self.priorities else 1.0
        
        experience = (state, action, reward, next_state, done)
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
            self.priorities.append(max_priority)
        else:
            # 替换
            min_idx = np.argmin(self.priorities)
            self.buffer[min_idx] = experience
            self.priorities[min_idx] = max_priority
            
    def sample(self, batch_size):
        # 计算采样概率
        priorities = np.array(self.priorities) ** self.alpha
        probs = priorities / priorities.sum()
        
        # 采样
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # 重要性采样权重
        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)
        weights = weights / weights.max()
        
        samples = [self.buffer[i] for i in indices]
        
        return samples, indices, weights
```

---

## 6. 实践框架

### 6.1 D4RL数据集

```python
import gym
import d4rl  # Offline RL标准数据集

def load_d4rl_dataset(env_name='maze2d-medium-v0'):
    """加载D4RL离线数据集"""
    env = gym.make(env_name)
    dataset = env.get_dataset()
    
    return {
        'observations': dataset['observations'],
        'actions': dataset['actions'],
        'rewards': dataset['rewards'],
        'next_observations': dataset['next_observations'],
        'dones': dataset['terminals']
    }
```

### 6.2 完整训练流程

```python
def train_offline_rl():
    """完整的离线RL训练流程"""
    # 1. 加载环境
    env = gym.make('ant-medium-v0')
    
    # 2. 加载离线数据集
    dataset = env.get_dataset()
    
    # 3. 初始化Agent (以CQL为例)
    agent = ConservativeQLearning(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.shape[0]
    )
    
    # 4. 训练循环
    for step in range(1000000):
        # 采样批次
        batch = sample_batch(dataset, batch_size=256)
        
        # 更新
        losses = agent.update(batch)
        
        # 定期评估
        if step % 10000 == 0:
            eval_return = evaluate(agent, env)
            print(f"Step {step}, Eval Return: {eval_return}")
```

---

## 参考文献

1. Levine, S., et al. (2020). Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. arXiv.
2. Kumar, A., et al. (2020). Conservative Q-Learning for Offline Reinforcement Learning. NeurIPS.
3. Fujita, Y., et al. (2021). Combating Selection Bias in Offline Reinforcement Learning. arXiv.

---

*本章节持续更新中...*
