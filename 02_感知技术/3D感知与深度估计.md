# 3D 感知与深度估计

3D感知使机器人能够理解三维空间结构，是导航、抓取、环境重建等任务的基础。本章介绍深度估计、3D目标检测、点云处理等技术。

## 目录

- [1. 深度估计](#1-深度估计)
- [2. 3D目标检测](#2-3d目标检测)
- [3. 点云处理](#3-点云处理)
- [4. 立体视觉](#4-立体视觉)
- [5. NeRF与3D重建](#5-nerf与3d重建)

---

## 1. 深度估计

### 1.1 传统方法

#### 三角测量

```
深度 Z = (f × 基线距离 B) / 视差 d
```

```python
import numpy as np

def depth_from_disparity(disparity, baseline=0.5, focal_length=500):
    """从视差计算深度"""
    # 避免除零
    disparity = np.where(disparity > 0, disparity, 0.001)
    depth = (baseline * focal_length) / disparity
    return depth
```

#### 双目立体匹配

```python
import cv2

def stereo_matching(left_img, right_img, method='sgbm'):
    """双目立体匹配"""
    if method == 'sgbm':
        # SGBM (Semi-Global Block Matching)
        window_size = 3
        min_disp = 0
        max_disp = 64
        
        stereo = cv2.StereoSGBM_create(
            minDisparity=min_disp,
            numDisparities=max_disp,
            blockSize=window_size,
            P1=8 * 3 * window_size**2,
            P2=32 * 3 * window_size**2,
            disp12MaxDiff=1,
            uniquenessRatio=10,
            speckleWindowSize=100,
            speckleRange=32
        )
        
        disparity = stereo.compute(left_img, right_img)
        return disparity.astype(np.float32) / 16.0
    
    elif method == 'bm':
        # Block Matching
        stereo = cv2.StereoBM_create(numDisparities=64, blockSize=15)
        disparity = stereo.compute(left_img, right_img)
        return disparity.astype(np.float32)
```

### 1.2 深度学习方法

#### Monodepth（单目深度估计）

```python
import torch
import torch.nn as nn

class Monodepth(nn.Module):
    def __init__(self, num_layers=18, pretrained=True):
        super(Monodepth, self).__init__()
        
        # 使用ResNet作为编码器
        resnet = torchvision.models.resnet18(pretrained=pretrained)
        self.encoder = nn.Sequential(*list(resnet.children())[:-2])
        
        # 解码器
        self.decoder = nn.Sequential(
            nn.Conv2d(512, 256, 1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2, mode='bilinear'),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2, mode='bilinear'),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2, mode='bilinear'),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()  # 输出0-1的深度概率
        )
        
    def forward(self, x):
        features = self.encoder(x)
        depth = self.decoder(features)
        return depth
```

#### MiDaS（多任务深度估计）

```python
def estimate_depth_midas(image):
    """使用MiDaS进行深度估计"""
    # MiDaS支持多种输入
    model = torch.hub.load('intel-isl/MiDaS', 'MiDaS')
    model.eval()
    
    # 预处理
    input_tensor = transform(image).unsqueeze(0)
    
    # 推理
    with torch.no_grad():
        depth = model(input_tensor)
    
    return depth
```

### 1.3 深度估计损失函数

```python
class DepthLoss(nn.Module):
    def __init__(self):
        super(DepthLoss, self).__init__()
        
    def forward(self, pred_depth, gt_depth):
        # 深度平滑损失
        smooth_loss = self.compute_smooth_loss(pred_depth)
        
        # 视差一致性损失 (如果是双帧)
        # ssim_loss = self.compute_ssim(pred_depth, gt_depth)
        
        # L1损失
        l1_loss = torch.abs(pred_depth - gt_depth).mean()
        
        # 边缘感知平滑
        edge_aware_smooth = self.edge_aware_smooth(pred_depth, gt_depth)
        
        return l1_loss + 0.1 * smooth_loss
    
    def compute_smooth_loss(self, depth):
        """深度平滑损失"""
        grad_x = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])
        grad_y = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])
        return grad_x.mean() + grad_y.mean()
    
    def edge_aware_smooth(self, depth, image):
        """边缘感知平滑损失"""
        # 在图像边缘处减少平滑约束
        pass
```

---

## 2. 3D目标检测

### 2.1 相机坐标系下的3D检测

#### 3D边界框表示

```
3D边界框 = (x, y, z, l, w, h, yaw)
其中:
- (x, y, z): 中心位置
- (l, w, h): 长、宽、高
- yaw: 偏航角
```

#### F-PointNet

```python
class FPointNet(nn.Module):
    def __init__(self, num_classes=3):
        super(FPointNet, self).__init__()
        
        # 2D检测器
        self.rcnn = RCNN2D()
        
        # 视锥生成
        self.frustum_proposal = FrustumProposal()
        
        # 特征提取
        self.pointnet = PointNetSetAbstraction(
            npoint=1024,
            radius=0.2,
            nsample=32,
            in_channel=3,
            mlp=[32, 32, 64],
            group_all=False
        )
        
        # 分类与回归头
        self.cls_head = nn.Linear(64, num_classes)
        self.reg_head = nn.Linear(64, 3 + 2)  # 中心偏移 + 尺寸
    
    def forward(self, image, point_cloud):
        # 2D检测
        box2d = self.rcnn(image)
        
        # 生成视锥
        frustum = self.frustum_proposal(box2d, image.shape)
        
        # 截取点云
        roi_points = self.crop_point_cloud(point_cloud, frustum)
        
        # 特征提取
        features, _ = self.pointnet(roi_points)
        
        # 预测
        cls_score = self.cls_head(features)
        bbox3d = self.reg_head(features)
        
        return cls_score, bbox3d
```

### 2.2 LiDAR 3D检测

#### PointPillars

```python
class PointPillars(nn.Module):
    def __init__(self, num_classes=3):
        super(PointPillars, self).__init__()
        
        # 点云支柱网络
        self.pillar_net = PillarNet(
            max_points=100,
            max_pillars=12000,
            grid_size=[0.16, 0.16, 4]
        )
        
        # 特征编码
        self.scn = SparseConv3d(64, 64)
        
        # RPN
        self.rpn = RPN()
        
        # 检测头
        self.detection_head = DetectionHead(num_classes)
        
    def forward(self, point_cloud):
        # 支柱化
        pillars, coords = self.pillar_net(point_cloud)
        
        # 特征提取
        features = self.scn(pillars)
        
        # RPN
        rpn_out = self.rpn(features)
        
        # 检测
        detections = self.detection_head(rpn_out)
        
        return detections
```

### 2.3 多模态3D检测

```python
class MultiModal3DDetection(nn.Module):
    def __init__(self):
        # 视觉分支
        self.visual_branch = Visual3DBranch()
        
        # LiDAR分支
        self.lidar_branch = LiDAR3DBranch()
        
        # 融合模块
        self.fusion = AttentionFusion(dim=256)
        
        # 检测头
        self.head = DetectionHead(num_classes=3)
        
    def forward(self, image, point_cloud):
        # 视觉3D检测
        visual_boxes = self.visual_branch(image)
        
        # LiDAR 3D检测
        lidar_boxes = self.lidar_branch(point_cloud)
        
        # 特征级融合
        fused_features = self.fusion(
            visual_boxes.features,
            lidar_boxes.features
        )
        
        # 最终检测
        final_boxes = self.head(fused_features)
        
        return final_boxes
```

---

## 3. 点云处理

### 3.1 点云基础操作

```python
import open3d as o3d
import numpy as np

class PointCloudProcessor:
    def __init__(self):
        pass
        
    def load_point_cloud(self, path):
        """加载点云"""
        pcd = o3d.io.read_point_cloud(path)
        return np.asarray(pcd.points)
    
    def downsample(self, points, voxel_size=0.05):
        """体素降采样"""
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        downpcd = pcd.voxel_down_sample(voxel_size)
        return np.asarray(downpcd.points)
    
    def remove_outliers(self, points, nb_neighbors=20, std_ratio=2.0):
        """去除离群点"""
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        
        cl, ind = pcd.remove_statistical_outlier(
            nb_neighbors=nb_neighbors,
            std_ratio=std_ratio
        )
        return np.asarray(pcd.points)[ind]
    
    def estimate_normals(self, points, radius=0.1):
        """估计法向量"""
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        pcd.estimate_normals(
            search_param=o3d.geometry.KDTreeSearchParamHybrid(
                radius=radius, max_nn=30
            )
        )
        return np.asarray(pcd.normals)
```

### 3.2 点云分割

```python
def point_cloud_segmentation(points, model="ransac", distance_threshold=0.2):
    """点云分割"""
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)
    
    if model == "ransac":
        # RANSAC平面分割
        plane_model, inliers = pcd.segment_plane(
            distance_threshold=distance_threshold,
            ransac_n=3,
            num_iterations=1000
        )
        
        # 平面内点
        inlier_points = np.asarray(pcd.points)[inliers]
        
        # 剩余点
        outlier_points = np.asarray(pcd.points)[np.delete(np.arange(len(points)), inliers)]
        
        return plane_model, inlier_points, outlier_points
    
    elif model == "dbscan":
        # DBSCAN聚类
        labels = np.array(pcd.cluster_dbscan(eps=0.5, min_points=10))
        return labels
```

### 3.3 点云配准

```python
def point_cloud_registration(source, target, method="icp"):
    """点云配准"""
    source_pcd = o3d.geometry.PointCloud()
    source_pcd.points = o3d.utility.Vector3dVector(source)
    
    target_pcd = o3d.geometry.PointCloud()
    target_pcd.points = o3d.utility.Vector3dVector(target)
    
    if method == "icp":
        # ICP配准
        threshold = 0.02
        trans_init = np.eye(4)
        
        reg_p2p = o3d.pipelines.registration.registration_icp(
            source_pcd, target_pcd, threshold, trans_init,
            o3d.pipelines.registration.TransformationEstimationPointToPoint()
        )
        return reg_p2p.transformation
    
    elif method == "feature":
        # 基于特征的配准
        # 提取FPFH特征
        source_fpfh = compute_fpfh(source_pcd)
        target_fpfh = compute_fpfh(target_pcd)
        
        # RANSAC配准
        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(
            source_pcd, target_pcd, source_fpfh, target_fpfh, True, 0.3
        )
        return result.transformation
```

---

## 4. 立体视觉

### 4.1 双目相机标定

```python
class StereoCalibration:
    def __init__(self):
        self.K1, self.D1 = None, None  # 左相机内参
        self.K2, self.D2 = None, None  # 右相机内参
        self.R, self.T = None, None    # 相对外参
        self.E, self.F = None, None    # 本征矩阵和基础矩阵
        
    def calibrate(self, left_images, right_images):
        """双目相机标定"""
        object_points = []
        left_image_points = []
        right_image_points = []
        
        # 提取角点
        for left_img, right_img in zip(left_images, right_images):
            # 分别标定左右相机
            ret1, mtx1, dist1, _, _ = cv2.calibrateCamera(...)
            ret2, mtx2, dist2, _, _ = cv2.calibrateCamera(...)
            
            # 双目标定
            ret, K1, D1, K2, D2, R, T, E, F = cv2.stereoCalibrate(
                object_points, left_image_points, right_image_points,
                mtx1, dist1, mtx2, dist2, gray.shape[::-1]
            )
            
        return K1, D1, K2, D2, R, T
```

### 4.2 视差计算优化

```python
def compute_disparity_optimized(left_img, right_img):
    """优化的视差计算"""
    # 预处理
    left_gray = cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY)
    right_gray = cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY)
    
    # 左右一致性检查
    # ... 实现左右一致性检查 ...
    
    # 视差后处理
    # 1. 亚像素拟合
    # 2. 斑点滤波
    # 3. 遮挡处理
    
    return disparity
```

---

## 5. NeRF与3D重建

### 5.1 NeRF基础

Neural Radiance Fields (NeRF) 使用神经网络表示3D场景：

```python
class NeRF(nn.Module):
    def __init__(self, D=8, W=256):
        super(NeRF, self).__init__()
        
        # 位置编码
        self.pos_encoding = PosEncoding(L=10)
        
        # 网络结构
        layers = []
        for i in range(D):
            if i == 0:
                layers.append(nn.Linear(60, W))  # 3*10*2 = 60
            else:
                layers.append(nn.Linear(W, W))
            layers.append(nn.ReLU())
        
        self.mlp = nn.Sequential(*layers)
        
        # 输出层
        self.sigma_head = nn.Linear(W, 1)  # 密度
        self.color_head = nn.Linear(W, 3)  # 颜色
        
    def forward(self, x, d):
        """
        x: 3D位置 (N, 3)
        d: 视角方向 (N, 3)
        """
        # 位置编码
        x_enc = self.pos_encoding(x)
        d_enc = self.pos_encoding(d)
        
        # MLP
        h = self.mlp(x_enc)
        
        # 输出
        sigma = self.sigma_head(h)
        color = torch.sigmoid(self.color_head(h))
        
        return color, sigma
```

### 5.2 3D Gaussian Splatting

```python
class Gaussian3D:
    """3D Gaussian Splatting 表示"""
    def __init__(self):
        self.positions = None      # 位置
        self.scales = None         # 缩放
        self.rotations = None      # 旋转
        self.opacities = None      # 不透明度
        self.colors = None         # 颜色
        self.sh_coeffs = None      # 球谐系数
        
    def render(self, camera):
        """光栅化渲染"""
        # 将3D高斯投影到2D
        # 计算每个像素的颜色
        pass
```

---

## 参考文献

1. Godard, C., et al. (2019). Digging Into Self-Supervised Monocular Depth Estimation. ICCV.
2. Qi, C. R., et al. (2018). Frustum PointNets for 3D Object Detection from RGB-D Data. CVPR.
3. Shi, S., et al. (2019). PointPillars: Fast Encoders for Object Detection from Point Clouds. CVPR.
4. Mildenhall, B., et al. (2020). NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ECCV.

---

*本章节持续更新中...*
