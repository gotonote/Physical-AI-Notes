# 实战教程：机器人控制算法实现

> 本章提供机器人控制算法的完整实现示例，从基础到进阶。

## 1. 环境准备

### 1.1 安装依赖

```bash
pip install numpy torch gymnasium pybullet matplotlib
```

### 1.2 基础环境配置

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple, Dict

# 设置随机种子
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)

set_seed(42)
```

---

## 2. 基础机器人仿真环境

### 2.1 使用 PyBullet 创建简单环境

```python
import pybullet as p
import pybullet_data
import numpy as np

class SimpleRobotEnv:
    """
    简单的2D机器人环境
    """
    def __init__(self, render=False):
        # 连接客户端
        if render:
            self.client = p.connect(p.GUI)
        else:
            self.client = p.connect(p.DIRECT)
        
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        
        # 加载环境
        self.plane_id = p.loadURDF("plane.urdf")
        self.robot_id = p.loadURDF("kuka_iiwa/model.urdf", [0, 0, 0])
        
        # 关节信息
        self.num_joints = p.getNumJoints(self.robot_id)
        self.joint_indices = [i for i in range(self.num_joints) 
                            if p.getJointInfo(self.robot_id, i)[2] != p.JOINT_FIXED]
        
        # 状态空间
        self.observation_dim = len(self.joint_indices) * 2  # 位置 + 速度
        self.action_dim = len(self.joint_indices)
        
    def reset(self) -> np.ndarray:
        """重置环境"""
        # 重置关节位置
        for i, idx in enumerate(self.joint_indices):
            p.resetJointState(self.robot_id, idx, 0)
        
        return self._get_obs()
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """执行动作"""
        # 简单PD控制
        for i, idx in enumerate(self.joint_indices):
            p.setJointMotorControl2(
                self.robot_id, idx,
                p.POSITION_CONTROL,
                targetPosition=action[i],
                force=100
            )
        
        p.stepSimulation()
        
        obs = self._get_obs()
        reward = self._compute_reward()
        done = False
        info = {}
        
        return obs, reward, done, info
    
    def _get_obs(self) -> np.ndarray:
        """获取观测"""
        obs = []
        for idx in self.joint_indices:
            state = p.getJointState(self.robot_id, idx)
            obs.extend([state[0], state[1]])  # 位置, 速度
        return np.array(obs, dtype=np.float32)
    
    def _compute_reward(self) -> float:
        """计算奖励"""
        # 简化的奖励函数
        return 0.0
    
    def close(self):
        p.disconnect(self.client)
```

---

## 3. 强化学习算法实现

### 3.1 PPO 算法

```python
class ReplayBuffer:
    """经验回放缓冲区"""
    def __init__(self, obs_dim, action_dim, max_size=10000):
        self.obs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)
        self.action_buf = np.zeros((max_size, action_dim), dtype=np.float32)
        self.reward_buf = np.zeros((max_size, 1), dtype=np.float32)
        self.next_obs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)
        self.done_buf = np.zeros((max_size, 1), dtype=np.float32)
        self.ptr = 0
        self.size = 0
        self.max_size = max_size
        
    def add(self, obs, action, reward, next_obs, done):
        self.obs_buf[self.ptr] = obs
        self.action_buf[self.ptr] = action
        self.reward_buf[self.ptr] = reward
        self.next_obs_buf[self.ptr] = next_obs
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)
    
    def sample(self, batch_size) -> Tuple:
        indices = np.random.randint(0, self.size, batch_size)
        return (
            self.obs_buf[indices],
            self.action_buf[indices],
            self.reward_buf[indices],
            self.next_obs_buf[indices],
            self.done_buf[indices]
        )


class ActorCritic(nn.Module):
    """Actor-Critic 网络"""
    def __init__(self, obs_dim, action_dim, hidden_dim=256):
        super().__init__()
        
        # 共享特征提取层
        self.feature = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actor: 策略网络
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()  # 输出 [-1, 1]
        )
        
        # Critic: 价值网络
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, obs):
        features = self.feature(obs)
        action_mean = self.actor(features)
        value = self.critic(features)
        return action_mean, value


class PPOAgent:
    """PPO 智能体"""
    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, 
                 clip_eps=0.2, epochs=10):
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.epochs = epochs
        
        # 网络
        self.policy = ActorCritic(obs_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
        # 缓冲区
        self.buffer = ReplayBuffer(obs_dim, action_dim)
        
    def select_action(self, obs, training=True):
        """选择动作"""
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
        
        with torch.no_grad():
            action_mean, value = self.policy(obs_tensor)
            
        if training:
            action = action_mean + torch.randn_like(action_mean) * 0.1
        else:
            action = action_mean
            
        return action.squeeze(0).numpy(), value.item()
    
    def update(self, batch_size=256):
        """更新策略"""
        # 收集足够数据
        if self.buffer.size < batch_size:
            return
        
        # 采样数据
        obs, action, reward, next_obs, done = self.buffer.sample(batch_size)
        
        obs = torch.FloatTensor(obs)
        action = torch.FloatTensor(action)
        reward = torch.FloatTensor(reward)
        next_obs = torch.FloatTensor(next_obs)
        done = torch.FloatTensor(done)
        
        # 计算GAE
        with torch.no_grad():
            _, values = self.policy(obs)
            _, next_values = self.policy(next_obs)
            
            td_target = reward + self.gamma * (1 - done) * next_values
            advantage = td_target - values
            
        # PPO 更新
        for _ in range(self.epochs):
            action_mean, values = self.policy(obs)
            
            # 策略损失
            ratio = torch.exp(action - action_mean)  # 简化
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantage
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # 价值损失
            value_loss = F.mse_loss(values, td_target.detach())
            
            # 更新
            self.optimizer.zero_grad()
            loss = policy_loss + 0.5 * value_loss
            loss.backward()
            self.optimizer.step()
    
    def train(self, env, num_episodes=500):
        """训练循环"""
        for episode in range(num_episodes):
            obs = env.reset()
            total_reward = 0
            
            while True:
                action, _ = self.select_action(obs)
                next_obs, reward, done, _ = env.step(action)
                
                self.buffer.add(obs, action, reward, next_obs, done)
                
                obs = next_obs
                total_reward += reward
                
                if done:
                    break
            
            # 更新
            self.update()
            
            if episode % 50 == 0:
                print(f"Episode {episode}, Reward: {total_reward:.2f}")


# 使用示例
if __name__ == "__main__":
    # 创建环境
    env = SimpleRobotEnv()
    
    # 创建智能体
    agent = PPOAgent(
        obs_dim=env.observation_dim,
        action_dim=env.action_dim,
        lr=3e-4
    )
    
    # 训练
    agent.train(env, num_episodes=100)
    
    env.close()
```

---

## 4. 模仿学习实现

### 4.1 行为克隆 (Behavior Cloning)

```python
class BehaviorCloning:
    """行为克隆"""
    def __init__(self, obs_dim, action_dim, hidden_dim=256):
        self.policy = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)
        
    def train(self, observations, actions, epochs=100):
        """训练"""
        observations = torch.FloatTensor(observations)
        actions = torch.FloatTensor(actions)
        
        for epoch in range(epochs):
            self.optimizer.zero_grad()
            
            pred_actions = self.policy(observations)
            loss = F.mse_loss(pred_actions, actions)
            
            loss.backward()
            self.optimizer.step()
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
                
    def predict(self, obs):
        """预测动作"""
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            action = self.policy(obs_tensor)
        return action.squeeze(0).numpy()


# 加载演示数据示例
def load_demonstration(file_path):
    """加载专家演示数据"""
    data = np.load(file_path)
    observations = data['observations']
    actions = data['actions']
    return observations, actions


# 训练行为克隆
if __name__ == "__main__":
    # 假设有演示数据
    # observations, actions = load_demonstration("demo.npy")
    
    # 模拟数据
    observations = np.random.randn(1000, 10).astype(np.float32)
    actions = np.random.randn(1000, 3).astype(np.float32)
    
    bc = BehaviorCloning(obs_dim=10, action_dim=3)
    bc.train(observations, actions, epochs=200)
```

---

## 5. 完整项目：机械臂抓取

### 5.1 项目结构

```
robot_grasp_project/
├── env/
│   └── grasp_env.py      # 抓取环境
├── models/
│   ├── actor_critic.py    # 策略网络
│   └── world_model.py    # 世界模型
├── agents/
│   └── ppo_agent.py      # PPO智能体
├── utils/
│   └── replay_buffer.py  # 工具函数
├── train.py              # 训练脚本
└── evaluate.py           # 评估脚本
```

### 5.2 训练脚本

```python
#!/usr/bin/env python3
"""
机械臂抓取训练脚本
"""

import numpy as np
import torch
from grasp_env import GraspEnv
from ppo_agent import PPOAgent
from replay_buffer import ReplayBuffer


def train():
    # 创建环境
    env = GraspEnv(render=True)
    
    # 创建智能体
    agent = PPOAgent(
        obs_dim=env.obs_dim,
        action_dim=env.action_dim,
        lr=3e-4,
        gamma=0.99
    )
    
    # 训练参数
    num_episodes = 2000
    max_steps = 500
    
    # 训练循环
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            # 选择动作
            action, _ = agent.select_action(obs)
            
            # 执行
            next_obs, reward, done, info = env.step(action)
            
            # 存储
            agent.buffer.add(obs, action, reward, next_obs, done)
            
            obs = next_obs
            episode_reward += reward
            
            if done:
                break
        
        # 更新策略
        agent.update()
        
        # 日志
        if episode % 10 == 0:
            print(f"Episode {episode}: Reward = {episode_reward:.2f}")
    
    # 保存模型
    torch.save(agent.policy.state_dict(), "policy.pth")
    print("Training complete!")
    
    env.close()


if __name__ == "__main__":
    train()
```

### 5.3 评估脚本

```python
#!/usr/bin/env python3
"""
评估脚本
"""

import torch
import numpy as np
from grasp_env import GraspEnv
from actor_critic import ActorCritic


def evaluate(policy_path, num_episodes=20):
    env = GraspEnv(render=True)
    
    # 加载策略
    policy = ActorCritic(env.obs_dim, env.action_dim)
    policy.load_state_dict(torch.load(policy_path))
    policy.eval()
    
    success_count = 0
    
    for episode in range(num_episodes):
        obs = env.reset()
        done = False
        
        while not done:
            with torch.no_grad():
                obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                action_mean, _ = policy(obs_tensor)
                action = action_mean.numpy()[0]
            
            obs, reward, done, info = env.step(action)
            
            if info.get('success', False):
                success_count += 1
                break
    
    success_rate = success_count / num_episodes
    print(f"Success Rate: {success_rate:.2%}")
    
    env.close()


if __name__ == "__main__":
    evaluate("policy.pth")
```

---

## 6. 进阶：使用 MuJoCo

```python
import mujoco
import mujoco.viewer

class MuJoCoRobot:
    """MuJoCo 机器人环境"""
    def __init__(self, model_path):
        # 加载模型
        self.model = mujoco.MjModel.from_xml_path(model_path)
        self.data = mujoco.MjData(self.model)
        
        # 启动可视化
        self.viewer = mujoco.viewer.launch_passive(self.model, self.data)
        
    def step(self, ctrl):
        """执行一步"""
        self.data.ctrl = ctrl
        mujoco.mj_step(self.model, self.data)
        
        # 获取观测
        obs = self._get_obs()
        reward = self._compute_reward()
        done = False
        
        return obs, reward, done, {}
    
    def _get_obs(self):
        """获取关节位置和速度"""
        return np.concatenate([
            self.data.qpos,  # 位置
            self.data.qvel   # 速度
        ])
    
    def reset(self):
        mujoco.mj_resetData(self.model, self.data)
        return self._get_obs()
    
    def close(self):
        self.viewer.close()
```

---

## 7. 调试与可视化

### 7.1 训练曲线绘制

```python
import matplotlib.pyplot as plt

def plot_training_curve(rewards, save_path="training_curve.png"):
    """绘制训练曲线"""
    plt.figure(figsize=(10, 5))
    
    # 平滑
    window = 50
    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
    
    plt.plot(smoothed)
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("Training Progress")
    plt.grid(True)
    plt.savefig(save_path)
    plt.show()
```

### 7.2 动作可视化

```python
def visualize_actions(actions, joint_names):
    """可视化关节动作"""
    plt.figure(figsize=(12, 4))
    
    for i, name in enumerate(joint_names):
        plt.subplot(1, len(joint_names), i+1)
        plt.plot(actions[:, i])
        plt.title(name)
        plt.xlabel("Time Step")
        plt.ylabel("Position")
    
    plt.tight_layout()
    plt.savefig("actions.png")
    plt.show()
```

---

## 8. 总结

```
┌─────────────────────────────────────────────────────────┐
│                   实战项目清单                           │
├─────────────────────────────────────────────────────────┤
│  ✓ 基础: PPO 机器人控制                                  │
│  ✓ 基础: 行为克隆                                        │
│  ✓ 进阶: 世界模型 + MBRL                                 │
│  ✓ 进阶: 扩散策略                                        │
│  ✓ 工具: PyBullet / MuJoCo 仿真                         │
│  ✓ 可视化: 训练曲线、动作轨迹                             │
└─────────────────────────────────────────────────────────┘
```

## 9. 下一步

- 尝试更多的机器人任务
- 实现 Dreamer/World Models
- 探索 Diffusion Policy
- 结合视觉输入

---

*祝学习愉快！*
