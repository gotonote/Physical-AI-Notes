# 传感器融合（LiDAR、Camera、Radar）

传感器融合将来自不同传感器的信息进行整合，以获得更准确、更鲁棒的环境感知结果。本章详细介绍各类传感器的原理及融合方法。

## 目录

- [1. 传感器概述](#1-传感器概述)
- [2. 相机-雷达融合](#2-相机-雷达融合)
- [3. LiDAR-相机融合](#3-lidar-相机融合)
- [4. 多传感器时间同步](#4-多传感器时间同步)
- [5. 空间对齐](#5-空间对齐)
- [6. 融合算法](#6-融合算法)

---

## 1. 传感器概述

### 1.1 各传感器特性对比

| 传感器 | 优点 | 缺点 | 典型应用 |
|--------|------|------|----------|
| Camera | 丰富的语义信息，成本低 | 受光照影响，无深度信息 | 目标分类、车道线检测 |
| LiDAR | 精确的深度信息，3D结构 | 成本高，无语义信息 | 3D目标检测、SLAM |
| Radar | 测速能力强，全天候 | 分辨率低，噪声大 | 目标跟踪、前方碰撞预警 |
| 超声波 | 成本极低，结构简单 | 探测距离短，精度低 | 泊车辅助 |

### 1.2 传感器选型

```
应用场景 → 传感器组合

低速室内导航：RGB-D相机 + 超声波
自动驾驶：LiDAR + 相机 + 毫米波雷达
工业检测：线激光 + 工业相机
```

---

## 2. 相机-雷达融合

### 2.1 融合架构

```
┌──────────────┐     ┌──────────────┐
│   相机检测   │     │   毫米波雷达  │
│  (2D bbox)   │     │   (目标列表)  │
└──────┬───────┘     └──────┬───────┘
       │                    │
       ▼                    ▼
┌────────────────────────────────┐
│         融合模块              │
│   - 空间投影                  │
│   - 目标关联                  │
│   - 状态融合                  │
└──────────────┬───────────────┘
               │
               ▼
        ┌──────────────┐
        │   融合结果   │
        │  (3D目标)    │
        └──────────────┘
```

### 2.2 相机-毫米波雷达融合代码

```python
import numpy as np
import cv2

class CameraRadarFusion:
    def __init__(self, camera_params, radar_params):
        # 相机参数
        self.K = camera_params['K']           # 内参矩阵
        self.R = camera_params['R']            # 外参旋转
        self.t = camera_params['t']            # 外参平移
        self.width = camera_params['width']
        self.height = camera_params['height']
        
        # 毫米波雷达参数
        self.radar_extrinsics = radar_params['extrinsics']
        
    def radar_to_camera(self, radar_objects):
        """
        将毫米波雷达目标投影到相机坐标系
        radar_objects: [[x, y, z, vx, vy], ...] (radar坐标系)
        """
        projected = []
        
        for obj in radar_objects:
            # 雷达坐标 (x, y, z)
            radar_coord = np.array([obj[0], obj[1], obj[2], 1])
            
            # 转换到相机坐标
            # 先转换到车辆坐标系，再到相机坐标系
            camera_coord = self.radar_extrinsics @ radar_coord
            
            # 投影到图像
            if camera_coord[2] > 0:  # 在相机前方
                uvw = self.K @ camera_coord[:3]
                u, v = uvw[0] / uvw[2], uvw[1] / uvw[2]
                
                if 0 <= u < self.width and 0 <= v < self.height:
                    projected.append({
                        'u': u,
                        'v': v,
                        'depth': camera_coord[2],
                        'vx': obj[3],
                        'vy': obj[4],
                        'range': np.sqrt(obj[0]**2 + obj[1]**2),
                        'doppler': obj[3]  # 多普勒速度
                    })
                    
        return projected
    
    def fuse(self, camera_detections, radar_objects, iou_threshold=0.3):
        """
        融合相机和雷达检测结果
        camera_detections: [{'bbox': [x1,y1,x2,y2], 'class': 'car', 'score': 0.9}, ...]
        radar_objects: 雷达目标列表
        """
        # 投影雷达目标到图像
        radar_projected = self.radar_to_camera(radar_objects)
        
        # 目标关联
        fused_objects = []
        
        for cam_det in camera_detections:
            cam_bbox = cam_det['bbox']
            cam_center = [(cam_bbox[0] + cam_bbox[2]) / 2, 
                          (cam_bbox[1] + cam_bbox[3]) / 2]
            
            # 找到最近的雷达目标
            best_match = None
            best_dist = float('inf')
            
            for radar_obj in radar_projected:
                dist = np.sqrt((radar_obj['u'] - cam_center[0])**2 + 
                              (radar_obj['v'] - cam_center[1])**2)
                
                if dist < best_dist and dist < 50:  # 像素距离阈值
                    best_dist = dist
                    best_match = radar_obj
            
            if best_match:
                # 融合：使用相机的分类 + 雷达的深度和速度
                fused_objects.append({
                    'bbox': cam_bbox,
                    'class': cam_det['class'],
                    'score': cam_det['score'],
                    'depth': best_match['depth'],
                    'velocity': np.sqrt(best_match['vx']**2 + best_match['vy']**2),
                    'source': 'fused'
                })
            else:
                # 无匹配，使用纯相机检测
                fused_objects.append({
                    'bbox': cam_bbox,
                    'class': cam_det['class'],
                    'score': cam_det['score'],
                    'depth': None,
                    'velocity': None,
                    'source': 'camera'
                })
        
        return fused_objects
```

---

## 3. LiDAR-相机融合

### 3.1 深度融合方案

#### 方案一：后融合（Late Fusion）

各传感器独立检测，决策层融合：

```python
class LateFusionLidarCamera:
    def __init__(self):
        self.detector_2d = TwoDDetector()  # 2D目标检测器
        self.detector_3d = ThreeDDetector()  # 3D目标检测器
        
    def fuse(self, image, point_cloud):
        # 独立检测
        dets_2d = self.detector_2d.detect(image)  # 2D bbox
        dets_3d = self.detector_3d.detect(point_cloud)  # 3D bbox
        
        # 匹配关联
        fused = self.match_and_fuse(dets_2d, dets_3d)
        
        return fused
    
    def match_and_fuse(self, dets_2d, dets_3d):
        """基于IoU的匹配融合"""
        fused = []
        
        for det_2d in dets_2d:
            best_iou = 0
            best_3d = None
            
            for det_3d in dets_3d:
                # 投影3D到2D并计算IoU
                iou = self.compute_iou_2d(det_2d.bbox, det_3d.bbox_2d)
                
                if iou > best_iou:
                    best_iou = iou
                    best_3d = det_3d
            
            if best_iou > 0.3:
                # 融合
                fused_obj = self.merge_detection(det_2d, best_3d)
                fused.append(fused_obj)
            else:
                fused.append(det_2d)
        
        return fused
```

#### 方案二：前融合（Early Fusion）

特征层融合：

```python
class EarlyFusionLidarCamera(nn.Module):
    def __init__(self):
        # 视觉分支
        self.visual_encoder = VisualEncoder()
        
        # LiDAR分支 (BEV特征)
        self.lidar_encoder = LiDAREncoderBEV()
        
        # 融合模块
        self.fusion_transformer = FusionTransformer(d_model=256)
        
        # 检测头
        self.detection_head = DetectionHead()
        
    def forward(self, image, point_cloud):
        # 视觉特征
        visual_feat = self.visual_encoder(image)  # (B, C, H, W)
        
        # LiDAR BEV特征
        lidar_feat = self.lidar_encoder(point_cloud)  # (B, C, H', W')
        
        # 调整尺寸对齐
        if visual_feat.shape[2:] != lidar_feat.shape[2:]:
            visual_feat = nn.functional.interpolate(
                visual_feat, size=lidar_feat.shape[2:]
            )
        
        # 特征融合
        fused_feat = self.fusion_transformer(visual_feat, lidar_feat)
        
        # 检测
        detections = self.detection_head(fused_feat)
        
        return detections
```

### 3.2 点云-图像投影

```python
def project_lidar_to_image(points_3d, K, R, t, image_shape):
    """
    将3D点云投影到2D图像
    
    points_3d: (N, 3) - LiDAR坐标系的点
    K: (3, 3) - 相机内参
    R, t: 相机外参
    """
    # 转换到相机坐标系
    points_cam = (R @ points_3d.T + t.reshape(3, 1)).T  # (N, 3)
    
    # 过滤在相机后方的点
    valid = points_cam[:, 2] > 0
    points_cam = points_cam[valid]
    
    # 投影到图像平面
    points_img = (K @ points_cam[:, :3].T).T  # (N, 3)
    uvw = points_img / points_img[:, 2:3]
    
    # 像素坐标
    u = uvw[:, 0]
    v = uvw[:, 1]
    
    # 过滤图像范围内的点
    h, w = image_shape
    in_image = (u >= 0) & (u < w) & (v >= 0) & (v < h)
    
    # 返回结果
    return {
        'u': u[in_image],
        'v': v[in_image],
        'depth': points_cam[in_image, 2],
        'intensity': points_3d[valid][in_image, 3] if points_3d.shape[1] > 3 else None
    }
```

---

## 4. 多传感器时间同步

### 4.1 硬件同步

```
传感器时间同步方案：
1. GPS/PPS时钟同步
2. 硬件触发同步
3. 软件时间戳对齐
```

### 4.2 软件时间同步

```python
import time
from collections import deque

class TimeSynchronizer:
    def __init__(self, tolerance=0.05):  # 50ms容差
        self.tolerance = tolerance
        self.buffers = {
            'camera': deque(),
            'lidar': deque(),
            'radar': deque()
        }
        
    def register(self, sensor_name, data, timestamp):
        """注册传感器数据"""
        self.buffers[sensor_name].append({
            'data': data,
            'timestamp': timestamp
        })
        
    def get_synchronized(self, timestamp):
        """获取同步数据"""
        synced = {}
        
        for sensor, buffer in self.buffers.items():
            if not buffer:
                continue
            
            # 找到最接近时间戳的数据
            best_idx = 0
            best_diff = float('inf')
            
            for i, item in enumerate(buffer):
                diff = abs(item['timestamp'] - timestamp)
                if diff < best_diff:
                    best_diff = diff
                    best_idx = i
            
            if best_diff <= self.tolerance:
                synced[sensor] = buffer[best_idx]['data']
                
                # 清理旧数据
                while best_idx > 0:
                    buffer.popleft()
                    best_idx -= 1
        
        return synced
```

---

## 5. 空间对齐

### 5.1 外参标定

```python
class ExtrinsicCalibration:
    def __init__(self):
        pass
    
    def calibrate_lidar_camera(self, lidar_points, camera_image, K):
        """
        LiDAR-相机外参标定
        方案：使用标定板
        """
        # 检测标定板在图像中的角点
        corners_2d = self.detect_checkerboard(camera_image)
        
        # 检测标定板在点云中的角点
        corners_3d = self.detect_lidar_corners(lidar_points)
        
        # PnP求解外参
        success, R, t = cv2.solvePnP(
            corners_3d, corners_2d, K, distCoeffs=None,
            flags=cv2.SOLVEPNP_ITERATIVE
        )
        
        return R, t
    
    def calibrate_radar_camera(self, radar_data, camera_image):
        """
        毫米波雷达-相机外参标定
        方案：使用角反
        """
        # 提取雷达中的静止目标（角反）
        static_targets = self.extract_static_targets(radar_data)
        
        # 图像中的角反位置
        corner_reflectors = self.detect_corner_reflectors(camera_image)
        
        # 求解变换矩阵
        # 使用EPnP或其他PnP变体
        pass
```

### 5.2 坐标变换

```python
import tf
import numpy as np

class CoordinateTransformer:
    def __init__(self):
        self.tf_listener = tf.TransformListener()
        
    def transform_point(self, point, from_frame, to_frame):
        """坐标变换"""
        # 创建点消息
        pt = PointStamped()
        pt.header.frame_id = from_frame
        pt.header.stamp = self.tf_listener.getLatestCommonTime(to_frame, from_frame)
        pt.point.x, pt.point.y, pt.point.z = point
        
        # 变换
        transformed = self.tf_listener.transformPoint(to_frame, pt)
        return [transformed.point.x, transformed.point.y, transformed.point.z]
    
    def lidar_to_camera_frame(self, lidar_point, T_lidar_to_camera):
        """LiDAR点转换到相机坐标"""
        point_h = np.append(lidar_point, 1)
        camera_point = T_lidar_to_camera @ point_h
        return camera_point[:3]
```

---

## 6. 融合算法

### 6.1 卡尔曼滤波融合

```python
class KalmanFusion:
    def __init__(self, state_dim=7, meas_dim=4):
        # 状态: [x, y, z, vx, vy, vz, w]
        self.kf = KalmanFilter(state_dim, meas_dim)
        
        # 状态转移矩阵
        self.kf.F = np.array([
            [1, 0, 0, 1, 0, 0, 0],  # x = x + vx
            [0, 1, 0, 0, 1, 0, 0],  # y = y + vy
            [0, 0, 1, 0, 0, 1, 0],  # z = z + vz
            [0, 0, 0, 1, 0, 0, 0],  # vx = vx
            [0, 0, 0, 0, 1, 0, 0],  # vy = vy
            [0, 0, 0, 0, 0, 1, 0],  # vz = vz
            [0, 0, 0, 0, 0, 0, 1]   # w = w
        ])
        
        # 观测矩阵 (相机只观测位置)
        self.kf.H = np.array([
            [1, 0, 0, 0, 0, 0, 0],
            [0, 1, 0, 0, 0, 0, 0],
            [0, 0, 1, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0, 1]
        ])
        
    def update_with_camera(self, bbox_2d, depth):
        """使用相机检测更新"""
        measurement = np.array([bbox_2d[0], bbox_2d[1], depth, bbox_2d[2]])
        self.kf.update(measurement)
        
    def update_with_radar(self, range, angle, doppler):
        """使用雷达更新"""
        # 雷达坐标转换
        x = range * np.cos(angle)
        y = range * np.sin(angle)
        
        measurement = np.array([x, y, doppler, 0])
        self.kf.update(measurement)
```

### 6.2 扩展卡尔曼滤波（EKF）

```python
class ExtendedKalmanFilter:
    def __init__(self):
        self.x = None  # 状态
        self.P = None  # 协方差
        
    def predict(self, dt):
        """预测步骤"""
        # 非线性状态转移
        self.x[3] += self.x[6] * dt  # vx
        self.x[4] += self.x[6] * dt  # vy
        
        # 雅可比矩阵
        F = np.eye(7)
        F[3, 6] = dt
        F[4, 6] = dt
        
        self.P = F @ self.P @ F.T + self.Q
        
    def update(self, z, sensor_type):
        """更新步骤"""
        # 观测函数
        if sensor_type == 'camera':
            h = np.array([self.x[0], self.x[1], self.x[2], self.x[6]])
        elif sensor_type == 'radar':
            h = np.array([
                np.sqrt(self.x[0]**2 + self.x[1]**2),
                np.arctan2(self.x[1], self.x[0]),
                self.x[3]
            ])
        
        # 雅可比矩阵
        H = self.compute_jacobian(sensor_type)
        
        # 更新
        y = z - h
        S = H @ self.P @ H.T + self.R
        K = self.P @ H.T @ np.linalg.inv(S)
        
        self.x = self.x + K @ y
        self.P = (np.eye(7) - K @ H) @ self.P
```

---

## 参考文献

1. Chen, X., et al. (2017). Multi-View 3D Object Detection Network for Autonomous Driving. CVPR.
2. Nobis, F., et al. (2019). A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection. IEEE.
3. Qi, C. R., et al. (2018). Frustum PointNets for 3D Object Detection from RGB-D Data. CVPR.

---

*本章节持续更新中...*
