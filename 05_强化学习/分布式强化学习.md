# 分布式强化学习

分布式强化学习通过并行收集经验、加速训练，是训练大规模机器人策略的关键技术。本章介绍分布式RL的架构、算法和实现。

## 目录

- [1. 分布式RL概述](#1-分布式rl概述)
- [2. 分布式架构](#2-分布式架构)
- [3. 经验收集器](#3-经验收集器)
- [4. 分布式训练算法](#4-分布式训练算法)
- [5. 通信机制](#5-通信机制)
- [6. 实现框架](#6-实现框架)

---

## 1. 分布式RL概述

### 1.1 为什么需要分布式RL

```
单智能体RL的问题:
- 样本效率低
- 训练时间长
- 探索受限

分布式RL的优势:
- 并行收集经验
- 加速训练
- 更好的探索
- 分布式计算资源
```

### 1.2 分布式RL分类

| 类型 | 描述 | 代表算法 |
|------|------|----------|
| A3C | 异步Actor-Critic | A3C, PAAC |
| PPO | 并行PPO | PPO-S, MPPI |
| 经验回放 | 分布式经验收集 | Ape-X |
| 模型基础 | 分布式规划 | DreamerV3 |

---

## 2. 分布式架构

### 2.1 经典架构

```
┌─────────────────────────────────────────────────────────────┐
│                    分布式RL架构                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│    ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐                  │
│    │Worker│  │Worker│  │Worker│  │Worker│  ← 环境交互     │
│    │  1   │  │  2   │  │  3   │  │  N   │                  │
│    └──┬───┘  └──┬───┘  └──┬───┘  └──┬───┘                  │
│       │        │        │        │                        │
│       └────────┴────────┴────────┘                        │
│                    │                                       │
│                    ▼                                       │
│    ┌───────────────────────────────┐                      │
│    │      经验缓冲 (Replay Buffer) │                      │
│    └───────────────┬───────────────┘                      │
│                    │                                       │
│                    ▼                                       │
│    ┌───────────────────────────────┐                      │
│    │         训练服务器             │  ← 参数更新        │
│    │      (Learner / Trainer)      │                      │
│    └───────────────┬───────────────┘                      │
│                    │                                       │
│                    ▼                                       │
│            参数同步/广播                                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 通信模式

```python
import numpy as np
from abc import ABC, abstractmethod

class CommunicationProtocol(ABC):
    """通信协议基类"""
    
    @abstractmethod
    def send(self, data, destination):
        """发送数据"""
        pass
    
    @abstractmethod
    def receive(self, source):
        """接收数据"""
        pass


class ParameterServer(CommunicationProtocol):
    """
    参数服务器架构
    中心化参数管理
    """
    def __init__(self, server_address="localhost:8000"):
        self.server_address = server_address
        self.parameters = {}  # 模型参数
        self.gradients = {}  # 梯度缓存
        
    def receive_gradients(self, worker_id, gradients):
        """接收Worker的梯度"""
        self.gradients[worker_id] = gradients
        
    def update_parameters(self, optimizer='adam'):
        """更新参数"""
        # 聚合梯度
        avg_gradients = self.average_gradients()
        
        # 更新参数
        self.parameters = self.apply_gradients(avg_gradients)
        
        return self.parameters
    
    def broadcast_parameters(self):
        """广播参数给所有Worker"""
        return self.parameters


class DecentralizedCommunication(CommunicationProtocol):
    """
    去中心化通信
    Worker之间直接通信
    """
    def __init__(self, worker_id, peer_ids):
        self.worker_id = worker_id
        self.peer_ids = peer_ids
        self.local_parameters = {}
        
    def send_to_peer(self, peer_id, data):
        """发送给对等节点"""
        pass
    
    def receive_from_peers(self):
        """从对等节点接收"""
        pass
```

---

## 3. 经验收集器

### 3.1 并行环境

```python
import multiprocessing as mp
from multiprocessing import Process, Queue
import numpy as np

class ParallelEnvironmentCollector:
    """
    并行环境收集器
    使用多进程并行收集经验
    """
    def __init__(self, env_fn, num_workers=8):
        self.num_workers = num_workers
        self.env_fn = env_fn
        self.queues = [Queue() for _ in range(num_workers)]
        self.processes = []
        
    def start(self):
        """启动收集进程"""
        for i in range(self.num_workers):
            p = Process(
                target=self.worker_loop,
                args=(i, self.env_fn, self.queues[i])
            )
            p.start()
            self.processes.append(p)
            
    def worker_loop(self, worker_id, env_fn, queue):
        """Worker进程循环"""
        env = env_fn()
        state = env.reset()
        
        while True:
            # 接收策略参数
            action = queue.get()
            
            # 执行动作
            next_state, reward, done, info = env.step(action)
            
            # 发送经验
            experience = (state, action, reward, next_state, done)
            queue.put(experience)
            
            if done:
                state = env.reset()
            else:
                state = next_state
                
    def collect(self, num_steps):
        """收集指定数量的步骤"""
        experiences = []
        
        for _ in range(num_steps):
            # 广播动作
            # ... 简化实现
            pass
            
        return experiences
    
    def stop(self):
        """停止所有进程"""
        for p in self.processes:
            p.terminate()
```

### 3.2 分布式Actor

```python
import torch
import torch.multiprocessing as mp

class DistributedActor(mp.Process):
    """
    分布式Actor
    每个Actor独立与环境交互并收集经验
    """
    def __init__(self, worker_id, env_fn, policy, replay_buffer, 
                 batch_size=32, update_freq=100):
        super().__init__()
        self.worker_id = worker_id
        self.env_fn = env_fn
        self.policy = policy  # 本地策略副本
        self.replay_buffer = replay_buffer
        self.batch_size = batch_size
        self.update_freq = update_freq
        
    def run(self):
        """运行Actor"""
        env = self.env_fn()
        state = env.reset()
        
        step_count = 0
        
        while True:
            # 选择动作
            action = self.policy.select_action(state)
            
            # 执行
            next_state, reward, done, _ = env.step(action)
            
            # 存储经验
            self.replay_buffer.push(
                state, action, reward, next_state, done
            )
            
            # 定期同步参数
            if step_count % self.update_freq == 0:
                self.sync_parameters()
                
            if done:
                state = env.reset()
            else:
                state = next_state
                
            step_count += 1
            
    def sync_parameters(self):
        """从中心服务器同步参数"""
        # 从parameter server拉
        #取最新参数 简化实现
        pass
```

---

## 4. 分布式训练算法

### 4.1 IMPALA架构

```python
import torch
import torch.nn as nn

class IMPALA:
    """
    IMPALA (Importance Weighted Actor-Learner Architecture)
    异步收集经验，批量学习
    """
    def __init__(self, state_dim, action_dim, num_actors=16):
        self.num_actors = num_actors
        
        # Learner (训练器)
        self.learner = Learner(state_dim, action_dim)
        
        # Actors (收集器)
        self.actors = [
            Actor(i, self.learner.model)
            for i in range(num_actors)
        ]
        
        # 轨迹缓冲
        self.trajectory_queue = mp.Queue()
        
    def train(self, num_steps):
        """训练循环"""
        # 启动Actors
        for actor in self.actors:
            actor.start()
            
        # Learner训练
        while True:
            # 从Actors收集轨迹
            trajectories = self.collect_trajectories()
            
            # 计算回报和优势
            for traj in trajectories:
                returns, advantages = self.compute_returns(traj)
                
            # 更新策略
            self.learner.update(trajectories)
            
    def compute_returns(self, trajectory, gamma=0.99, lambda_=0.95):
        """计算回报和优势 (V-trace)"""
        returns = []
        advantages = []
        
        # 简化的GAE计算
        gae = 0
        next_value = 0
        
        for t in reversed(range(len(trajectory))):
            value = trajectory[t]['value']
            reward = trajectory[t]['reward']
            
            delta = reward + gamma * next_value - value
            gae = delta + gamma * lambda_ * gae
            
            advantages.insert(0, gae)
            returns.insert(0, gae + value)
            
            next_value = value
            
        return returns, advantages
```

### 4.2 Ape-X架构

```python
class ApeX:
    """
    Ape-X: 分布式经验回放
    多个Actor并行收集，集中式优先级回放
    """
    def __init__(self, state_dim, action_dim, num_actors=8):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_actors = num_actors
        
        # 优先级经验回放
        self.replay_buffer = PrioritizedReplayBuffer(capacity=1000000)
        
        # 多个Learner
        self.learners = [Learner(state_dim, action_dim) for _ in range(4)]
        
        # 多个Actor
        self.actors = [
            Actor(i, self.learners[i % len(self.learners)], self.replay_buffer)
            for i in range(num_actors)
        ]
        
    def train(self):
        """Ape-X训练"""
        # 启动Actors
        for actor in self.actors:
            actor.start()
            
        # 分布式学习
        for learner in self.learners:
            learner.start()
            
        # 主循环
        while True:
            # Learners持续学习
            pass
```

### 4.3 Seed RL

```python
class SeedRL:
    """
    Seed RL: 统一的分布式架构
    核心思想：Learner在中心，Actors只负责数据收集
    """
    def __init__(self, state_dim, action_dim, num_actors=16):
        # 共享模型
        self.model = ActorCritic(state_dim, action_dim)
        
        # 分布式Actors
        self.actors = [
            SeedActor(i, self.model, self.get_inference_server())
            for i in range(num_actors)
        ]
        
    def get_inference_server(self):
        """推理服务器"""
        # 使用Ray或gRPC提供推理服务
        pass
```

---

## 5. 通信机制

### 5.1 参数同步

```python
class ParameterSynchronizer:
    """
    参数同步器
    管理分布式训练中的参数同步
    """
    def __init__(self, sync_mode='async'):
        self.sync_mode = sync_mode
        self.local_version = 0
        self.server_version = 0
        
    def sync_from_server(self):
        """从服务器同步参数"""
        if self.sync_mode == 'sync':
            # 同步：等待所有Worker
            self.wait_for_workers()
            return self.pull_parameters()
            
        elif self.sync_mode == 'async':
            # 异步：直接拉取
            return self.pull_parameters()
            
        elif self.sync_mode == 'lag':
            # 带延迟的同步
            if self.server_version - self.local_version > 10:
                return self.pull_parameters()
                
    def push_to_server(self, gradients):
        """推送梯度到服务器"""
        pass
```

### 5.2 高效通信

```python
class EfficientCommunicator:
    """
    高效通信优化
    - 梯度压缩
    - 量化
    - 稀疏化
    """
    def __init__(self, compress=True, quantize=True):
        self.compress = compress
        self.quantize = quantize
        
    def compress_gradients(self, gradients, compression=0.01):
        """梯度压缩"""
        # Top-K稀疏化
        flat_grad = gradients.flatten()
        
        # 保留最大的K个元素
        k = int(len(flat_grad) * compression)
        threshold = np.sort(np.abs(flat_grad))[-k]
        
        mask = np.abs(flat_grad) >= threshold
        sparse_grad = flat_grad * mask
        
        return sparse_grad, mask
    
    def quantize_parameters(self, params, bits=8):
        """参数量化"""
        # 简单的均匀量化
        min_val = params.min()
        max_val = params.max()
        
        num_bins = 2 ** bits
        
        # 量化
        quantized = ((params - min_val) / (max_val - min_val) * num_bins).round()
        
        # 反量化
        dequantized = quantized / num_bins * (max_val - min_val) + min_val
        
        return dequantized
```

---

## 6. 实现框架

### 6.1 Ray RLlib

```python
import ray
import ray.rllib as rllib
from ray.rllib.agents import ppo

def train_with_rllib():
    """使用Ray RLlib进行分布式训练"""
    # 初始化Ray
    ray.init(num_cpus=64)
    
    # 配置PPO
    config = {
        'env': 'HalfCheetah-v2',
        'num_workers': 32,  # 并行Worker数
        'num_gpus': 4,      # GPU数
        'lr': 0.001,
        'train_batch_size': 32000,
        'rollout_fragment_length': 200,
        
        # 分布式特定配置
        'num_envs_per_worker': 5,
        'remote_worker_envs': True,
    }
    
    # 创建Trainer
    trainer = ppo.PPOTrainer(config=config)
    
    # 训练
    for i in range(1000):
        result = trainer.train()
        
        if i % 100 == 0:
            print(result)
            
    ray.shutdown()
```

### 6.2 分布式PPO

```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DistributedPPO:
    """
    分布式PPO实现
    使用PyTorch Distributed
    """
    def __init__(self, state_dim, action_dim, num_processes):
        self.num_processes = num_processes
        
        # 模型
        self.policy = ActorCritic(state_dim, action_dim)
        self.policy = DDP(self.policy)
        
        # 优化器
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)
        
    def reduce_gradients(self):
        """梯度同步"""
        for param in self.policy.parameters():
            dist.all_reduce(param.grad.data)
            param.grad.data /= self.num_processes
            
    def broadcast_parameters(self):
        """参数广播"""
        for param in self.policy.parameters():
            dist.broadcast(param.data, src=0)
```

---

## 参考文献

1. Mnih, V., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. ICML.
2. Espeholt, L., et al. (2018). IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. ICML.
3. Horgan, D., et al. (2018). ApeX: A Scalable Architecture for Data-Efficient Parallel Reinforcement Learning. arXiv.

---

*本章节持续更新中...*
