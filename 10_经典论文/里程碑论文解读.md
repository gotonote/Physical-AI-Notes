# 经典论文解读

> 本章整理物理AI/具身智能领域的里程碑论文，提供核心思想和关键贡献的解读。

## 1. 强化学习篇

### 1.1 PPO (Proximal Policy Optimization)

**论文**: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)

**核心思想**: 通过裁剪策略更新幅度，避免策略剧烈变化。

```python
# PPO 核心损失
def ppo_loss(log_pi, old_log_pi, advantage, clip_eps=0.2):
    """
    PPO 裁剪目标
    """
    ratio = torch.exp(log_pi - old_log_pi)
    
    # 裁剪
    clipped_ratio = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps)
    
    # 取最小值
    loss = -torch.min(ratio * advantage, clipped_ratio * advantage)
    
    return loss.mean()
```

**关键贡献**:
- Clipped surrogate objective
- 简单的实现方式
- 稳定的训练过程

---

### 1.2 SAC (Soft Actor-Critic)

**论文**: Haarnoja et al., "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL" (2018)

**核心思想**: 最大熵强化学习，同时优化策略和熵。

$$\pi^* = \arg\max_\pi \mathbb{E}_{\tau \sim \pi}[R(\tau) + \alpha H(\pi(\cdot|s))]$$

```python
# SAC 软价值函数更新
def sac_update(q_network, target_q, optimizer, batch, alpha=0.2):
    obs, action, reward, next_obs, done = batch
    
    # 当前Q值
    q_value = q_network(obs, action)
    
    # 目标Q (使用双Q网络技巧)
    next_action, log_prob = policy.sample(next_obs)
    target_q = target_q(next_obs, next_action) - alpha * log_prob
    
    # 损失
    q_loss = F.mse_loss(q_value, target_q.detach())
    
    return q_loss
```

**关键贡献**:
- 最大熵框架
- 自动温度调整
- 稳定的连续控制

---

## 2. 模仿学习篇

### 2.1 DAgger (Dataset Aggregation)

**论文**: Ross et al., "A Reduction of Imitation Learning to No-Regret Online Learning" (2011)

**核心思想**: 迭代聚合专家数据，解决分布偏移问题。

```
┌─────────────────────────────────────────────────────────┐
│                    DAgger 算法流程                       │
│                                                         │
│   1. 收集专家演示 D = {(o_t, a_t^expert)}              │
│   2. 训练策略 π_θ(a|o) from D                          │
│   3. 运行 π_θ，收集轨迹 {(o_t, a_t)}                   │
│   4. 询问专家获取正确动作 a_t^expert                     │
│   5. 聚合数据 D = D ∪ {(o_t, a_t^expert)}              │
│   6. 重复 2-5                                           │
└─────────────────────────────────────────────────────────┘
```

**关键贡献**:
- 解决因果混淆问题
- 理论保证
- 简单有效

---

### 2.2 GAIL (Generative Adversarial Imitation Learning)

**论文**: Ho & Ermon, "Generative Adversarial Imitation Learning" (2016)

**核心思想**: 用GAN框架学习策略，判别器区分专家和策略生成的数据。

```python
class GAIL:
    def __init__(self, policy, discriminator):
        self.policy = policy
        self.discriminator = discriminator
        
    def discriminator_loss(self, expert_obs, expert_action, policy_obs, policy_action):
        """
        判别器损失
        """
        expert_pairs = torch.cat([expert_obs, expert_action], dim=-1)
        policy_pairs = torch.cat([policy_obs, policy_action], dim=-1)
        
        expert_logits = self.discriminator(expert_pairs)
        policy_logits = self.discriminator(policy_pairs)
        
        loss = F.binary_cross_entropy_with_logits(
            expert_logits, torch.ones_like(expert_logits)
        ) + F.binary_cross_entropy_with_logits(
            policy_logits, torch.zeros_like(policy_logits)
        )
        return loss
```

**关键贡献**:
- 端到端学习
- 不需要显式奖励函数
- 可以从少量专家数据学习

---

## 3. 世界模型篇

### 3.1 World Models

**论文**: Ha & Schmidhuber, "World Models" (2018)

**架构**: VAE + MDN-RNN

```
观测图像 ──(VAE)──> 隐向量 z
                  │
                  v
        (RNN处理序列) 预测下一隐状态
                  │
                  v
              隐空间动态学习
```

**关键贡献**:
- 首次在压缩隐空间学习世界模型
- 展现了"快速权重"思想
- 为后续Dreamer等奠定基础

---

### 3.2 Dreamer

**论文**: Hafner et al., "Dream to Control: Learning Behaviors by Latent Imagination" (2020)

**核心创新**:
- 变分自编码器 + RSSM
- 想象 rollout
- 演员-评论家架构

```python
# Dreamer 核心循环
def dreamer_update(model, batch):
    # 1. 世界模型学习
    obs_emb = model.encoder(obs)
    post = model.posterior(obs_emb)
    prior = model.prior(action)
    dyn_loss = kl_loss(post, prior)
    rec_loss = mse_loss(model.decoder(post), obs)
    
    # 2. 想象 rollout
    imagined = model.imagine(post, actions, horizon)
    
    # 3. 策略学习
    policy_loss = -value(imagined).mean()
    
    return dyn_loss + rec_loss + policy_loss
```

---

## 4. 多模态大模型篇

### 4.1 RT-2 (Robotics Transformer 2)

**论文**: Brohan et al., "RT-2: Vision-Language-Action Models" (2023)

**核心思想**: 将视觉-语言模型直接输出机器人动作。

```
┌─────────────────────────────────────────────────────────┐
│                      RT-2 架构                           │
│                                                         │
│   输入:                                                  │
│   ┌─────────┐    ┌─────────┐                          │
│   │ 图像    │ +  │ 文本指令│                          │
│   └─────────┘    └─────────┘                          │
│        │               │                               │
│        v               v                               │
│   ┌─────────────────────────────┐                       │
│   │    VLA (Vision-Language-Action)                   │
│   │    大模型微调               │                       │
│   └─────────────────────────────┘                       │
│               │                                          │
│               v                                          │
│   ┌─────────────────────────────┐                       │
│   │    动作 tokens (末端+关节)   │                       │
│   └─────────────────────────────┘                       │
└─────────────────────────────────────────────────────────┘
```

**关键贡献**:
- 泛化能力大幅提升
- 语义推理能力
- 零样本迁移

---

### 4.2 PaLM-E

**论文**: Driess et al., "PaLM-E: An Embodied Multimodal Language Model" (2023)

**核心创新**: 多模态 embedding 进入语言模型。

---

## 5. 扩散策略篇

### 5.1 Diffusion Policy

**论文**: Chi et al., "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion" (2023)

**核心公式**:

$$a_0 = \text{Denoise}(o_t, \epsilon_\theta)$$

```python
# 扩散策略采样
@torch.no_grad()
def get_action_diffusion(policy, obs, num_steps=10):
    action = torch.randn_like(action_dim)
    
    for t in reversed(range(num_steps)):
        noise_pred = policy(obs, action, t)
        action = (action - noise_pred) / np.sqrt(1 - alpha[t])
        
    return action
```

**关键贡献**:
- 动作生成的质量高
- 建模复杂多模态分布
- 稳定的训练

---

## 6. 仿真与数据收集

### 6.1 SIMPLER

**论文**: "SIMPLER: Single-Image Policy Learning with RGB Cameras and Manipulators" (2023)

---

### 6.2 MT-OPT

**论文**: "MT-OPT: Multi-Task Optical Perception" (2022)

---

## 7. 论文阅读建议

### 7.1 论文结构速读法

```
1. Abstract (摘要) - 核心贡献
2. Introduction (引言) - 问题定义 + 方法概述
3. Method (方法) - 核心技术细节
4. Experiment (实验) - 效果验证
5. Conclusion (结论) - 总结
```

### 7.2 代码复现路径

```python
# 推荐复现顺序
paper_reading_order = [
    "PPO",          # 强化学习基础
    "SAC",          # 连续控制
    "DAgger",      # 模仿学习基础
    "GAIL",        # 对抗模仿学习
    "World Models", # 世界模型基础
    "Dreamer",     # 世界模型进阶
    "Diffusion Policy", # 扩散策略
    "RT-2",        # VLA
]
```

---

## 8. 总结表格

| 类别 | 论文 | 年份 | 核心贡献 |
|------|------|------|----------|
| 强化学习 | PPO | 2017 | 裁剪策略更新 |
| 强化学习 | SAC | 2018 | 最大熵RL |
| 模仿学习 | DAgger | 2011 | 数据聚合 |
| 模仿学习 | GAIL | 2016 | 对抗学习 |
| 世界模型 | World Models | 2018 | VAE+RNN |
| 世界模型 | Dreamer | 2020 | 隐空间想象 |
| VLA | RT-2 | 2023 | 视觉语言动作 |
| 扩散策略 | Diffusion Policy | 2023 | 动作生成 |

---

*更多论文持续更新中...*
