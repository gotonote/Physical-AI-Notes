# 最新论文

Physical AI领域的前沿研究成果。

## 目录

- [1. 大模型与机器人](#1-大模型与机器人)
- [2. 强化学习](#2-强化学习)
- [3. 模仿学习](#3-模仿学习)
- [4. 感知技术](#4-感知技术)

---

## 1. 大模型与机器人

### 1.1 2024-2025重要论文

| 论文 | 机构 | 贡献 |
|------|------|------|
| RT-4 | Google DeepMind | 视觉语言动作模型 |
| OpenVLA | Stanford | 开源VLA |
|π0 | Physical Intelligence | 流体动作模型 |

### 1.2 核心进展

- **多模态理解**: 视觉+语言+触觉融合
- **长程推理**: Chain-of-thought应用于机器人
- **泛化能力**: 跨任务、跨实体泛化

### 1.3 VLA模型架构示例

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoProcessor

class VLAModel(nn.Module):
    """视觉语言动作模型基类"""
    
    def __init__(self, model_name="google/robot-vla"):
        super().__init__()
        self.vision_encoder = AutoModel.from_pretrained(
            "google/siglip-so-400m-patch14-224"
        )
        self.text_encoder = AutoModel.from_pretrained(
            "google/bert-base-uncased"
        )
        self.action_head = nn.Linear(768, 7)  # 7DOF动作
        
    def forward(self, image, text, state=None):
        # 视觉特征
        vision_outputs = self.vision_encoder(pixel_values=image)
        vision_embeds = vision_outputs.last_hidden_state
        
        # 文本特征
        text_outputs = self.text_encoder(input_ids=text)
        text_embeds = text_outputs.last_hidden_state
        
        # 跨模态融合
        fused = vision_embeds * text_embeds  # 元素级乘法
        
        # 动作预测
        action = self.action_head(fused.mean(dim=1))
        
        return action
    
    def predict_action(self, image, instruction, current_state):
        """推理时的动作预测"""
        with torch.no_grad():
            # 编码
            image_enc = self.vision_encoder(image)
            text_enc = self.text_encoder(instruction)
            
            # 融合
            fused = image_enc.last_hidden_state * text_enc.last_hidden_state
            
            # 预测动作
            action = self.action_head(fused.mean(dim=1))
            
        return action
```

### 1.4 π0 模型特点

```python
class PiZeroModel(nn.Module):
    """
    π0 流体动作模型
    特点: 预训练 + 后训练的两阶段方法
    """
    
    def __init__(self):
        super().__init__()
        # 预训练阶段: 互联网视频理解
        self.pretrain_encoder = SelfSupervisedEncoder()
        
        # 后训练阶段: 机器人动作
        self.action_head = nn.Sequential(
            nn.Linear(768, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 14),  # 7个关节 + 7个速度
        )
        
    def forward(self, image_sequence, text_instruction):
        # 处理图像序列
        batch_size, seq_len = image_sequence.shape[:2]
        
        # 提取时空特征
        features = self.pretrain_encoder(
            images=image_sequence.view(-1, *image_sequence.shape[2:])
        )
        features = features.view(batch_size, seq_len, -1)
        
        # 动作预测
        action = self.action_head(features[:, -1])
        
        return action
```

---

## 2. 强化学习

### 2.1 Sim-to-Real

- 域随机化技术成熟
- 自适应领域适应
- 少样本迁移

### 2.2 域随机化代码示例

```python
class DomainRandomization:
    """域随机化 - Sim-to-Real迁移"""
    
    def __init__(self):
        self.param_ranges = {
            'friction': (0.3, 1.5),
            'mass': (0.5, 2.0),
            'link_damping': (0.0, 0.5),
            'visual_color': None,  # 颜色随机
            'light_position': None,
            'texture_randomization': True,
        }
        
    def randomize(self, env):
        """随机化仿真环境参数"""
        # 物理参数
        for body in env.sim.model.body_mass:
            body_mass *= np.random.uniform(*self.param_ranges['mass'])
            
        for geom in env.sim.model.geom_friction:
            geom_friction *= np.random.uniform(*self.param_ranges['friction'])
            
        # 视觉 随机纹理参数 -
        if self.param_ranges['texture_randomization']:
            self._randomize_textures(env)
            
        return env
    
    def _randomize_textures(self, env):
        """随机化物体纹理"""
        texture_pool = ['metal', 'wood', 'plastic', 'rubber']
        for mat_id in env.sim.model.mat_id:
            mat_id.texture_id = np.random.choice(texture_pool)
```

### 2.3 离线RL

- CQL/BCQ工业应用
- 数据高效算法
- 安全约束学习

### 2.4 CQL算法实现

```python
class ConservativeQLearning:
    """
    CQL (Conservative Q-Learning)
    核心思想: 最小化Q值，避免过度乐观
    """
    
    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=0.005):
        self.gamma = gamma
        self.alpha = alpha  # 保守系数
        
        self.q_network = QNetwork(state_dim, action_dim)
        self.q_target = QNetwork(state_dim, action_dim)
        self.q_target.load_state_dict(self.q_network.state_dict())
        
    def compute_cql_loss(self, batch):
        """计算CQL损失"""
        states, actions, rewards, next_states, dones = batch
        
        # 标准Q学习损失
        current_q = self.q_network(states, actions)
        with torch.no_grad():
            next_q = self.q_target(next_states).max(dim=1)[0]
            target_q = rewards + self.gamma * (1 - dones) * next_q
            
        standard_loss = F.mse_loss(current_q.squeeze(), target_q)
        
        # CQL额外损失: 鼓励Q值保守
        # 采样_actions ~ Uniform(a)
        sampled_actions = torch.rand(states.shape[0], actions.shape[1]) * 2 - 1
        sampled_q = self.q_network(states, sampled_actions)
        
        # 最大化采样的Q值，然后取负（最小化）
        cql_loss = -self.alpha * (sampled_q.mean() - current_q.mean())
        
        return standard_loss + cql_loss
```

---

## 3. 模仿学习

### 3.1 扩散策略

- Diffusion Policy (RSS 2023)
- 工业应用案例
- 实时推理优化

### 3.2 扩散策略代码

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DiffusionPolicy(nn.Module):
    """扩散策略 - 基于去噪过程的策略学习"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256, num_steps=100):
        super().__init__()
        self.num_steps = num_steps
        
        # 时间步嵌入
        self.time_mlp = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
        # 噪声预测网络
        self.noise_pred_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, action_dim),
        )
        
    def forward(self, state, noisy_action, t):
        """预测噪声"""
        t_emb = self.time_mlp(t.float().unsqueeze(-1))
        x = torch.cat([state, noisy_action], dim=-1)
        noise_pred = self.noise_pred_net(x + t_emb)
        return noise_pred
    
    def sample_actions(self, state, num_samples=10):
        """从噪声开始，逐步去噪得到动作"""
        batch_size = state.shape[0]
        
        # 初始噪声
        action = torch.randn(batch_size, self.action_dim, device=state.device)
        
        # 逐步去噪
        for t in reversed(range(self.num_steps)):
            t_tensor = torch.full((batch_size,), t / self.num_steps, device=state.device)
            
            with torch.no_grad():
                noise_pred = self.forward(state, action, t_tensor)
                
            # 去噪步骤 (简化版DDPM)
            alpha = (t + 1) / self.num_steps
            action = action - (1 - alpha) * noise_pred / torch.sqrt(alpha)
            
        return action
```

### 3.3 数据集

- Open X-Embodiment
- DROID
- 互联网机器人数据

### 3.4 数据加载示例

```python
import h5py
from torch.utils.data import Dataset

class RobotDataset(Dataset):
    """机器人演示数据集加载"""
    
    def __init__(self, dataset_path):
        self.data = h5py.File(dataset_path, 'r')
        
    def __len__(self):
        return len(self.data['observations'])
    
    def __getitem__(self, idx):
        # 观测
        obs = {
            'image': self.data['observations']['image'][idx],
            'state': self.data['observations']['state'][idx],
            'instruction': self.data['observations']['language_instruction'][idx].decode(),
        }
        
        # 动作
        action = self.data['actions'][idx]
        
        return obs, action
    
    def get_episode(self, episode_id):
        """获取一个完整episode"""
        ep_start = self.data['episodestarts'][episode_id]
        ep_end = self.data['episodeends'][episode_id]
        
        observations = self.data['observations'][ep_start:ep_end]
        actions = self.data['actions'][ep_start:ep_end]
        
        return observations, actions
```

---

## 4. 感知技术

### 4.1 3D感知

- 实时NeRF
- 神经RGBD
- 端到端感知

### 4.2 神经RGBD重建

```python
class NeuralRGBD(nn.Module):
    """神经RGBD - 结合神经渲染的深度估计"""
    
    def __init__(self):
        super().__init__()
        
        # 深度估计网络
        self.depth_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2),
            nn.ReLU(),
            *self._make_encoder_block(64, 128),
            *self._make_encoder_block(128, 256),
            *self._make_encoder_block(256, 512),
        )
        
        self.depth_decoder = nn.Sequential(
            *self._make_decoder_block(512, 256),
            *self._make_decoder_block(256, 128),
            *self._make_decoder_block(128, 64),
            nn.Conv2d(64, 1, 3, padding=1),
            nn.Sigmoid(),  # 深度归一化到[0,1]
        )
        
        # NeRF颜色预测
        self.nerf_head = nn.Sequential(
            nn.Linear(128 + 3 + 2),  # feature + position + direction
            nn.ReLU(),
            nn.ReLU(),
            nn.Linear(128, 4),  # RGB + density
        )
        
    def forward(self, image):
        # 深度估计
        features = self.depth_encoder(image)
        depth = self.depth_decoder(features)
        
        # 神经渲染
        color, density = self.nerf_render(features, image.shape[2:])
        
        return depth, color, density
    
    def _make_encoder_block(self, in_ch, out_ch):
        return [
            nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.ReLU(),
        ]
    
    def _make_decoder_block(self, in_ch, out_ch):
        return [
            nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1),
            nn.ReLU(),
        ]
```

---

## 参考文献

1. Brohan, A., et al. (2024). RT-2: Vision-Language-Action Models. arXiv.
2. Black, K., et al. (2024). π0: A Vision-Language-Action Flow Model. Physical Intelligence.
3. Haarnoja, T., et al. (2024). Diffusion Policies for Real-World Robot Learning. RSS.
4. Kumar, A., et al. (2021). Conservative Q-Learning for Offline RL. NeurIPS.

---

*本章节持续更新中...*
