# 世界模型概论

> 本章介绍世界模型的基本概念，它是物理AI系统中构建智能决策的核心组件。

## 1. 什么是世界模型？

### 1.1 定义

**世界模型（World Model）** 是智能体对外部环境的内部表示和预测模型。

```
┌─────────────────────────────────────────────────────────────┐
│                     世界模型在智能系统中的位置                │
│                                                             │
│   ┌─────────┐      ┌─────────┐      ┌─────────┐           │
│   │  感知   │  ->  │ 世界模型 │  ->  │   决策   │           │
│   │ Perception│     │World Model│     │  Policy  │           │
│   └─────────┘      └─────────┘      └─────────┘           │
│        │                 │                 │              │
│        v                 v                 v              │
│     观测 o           状态 s           动作 a              │
│                   预测 s'                               │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 世界模型的核心功能

| 功能 | 描述 | 示例 |
|------|------|------|
| 状态表征 | 将感知信息编码为紧凑状态 | CNN编码观测 |
| 转移预测 | 预测下一状态 | $s_{t+1} = f(s_t, a_t)$ |
| 奖励预测 | 预测即时奖励 | $r_t = g(s_t, a_t)$ |
| 想象推演 | 在隐空间模拟未来 | 想象序列展开 |

## 2. 世界模型的发展历程

### 2.1 传统世界模型

- **POMDP**: 部分可观测马尔可夫决策过程
- **MDP**: 马尔可夫决策过程
- **卡尔曼滤波**: 线性系统状态估计

### 2.2 深度世界模型

| 年份 | 模型 | 贡献 |
|------|------|------|
| 2018 | World Models | 首次使用VAE+RNN构建世界模型 |
| 2019 | PlaNet | 基于模型的强化学习 |
| 2020 | Dreamer | 想象空间中学习 |
| 2022 | VI-Projects | 视觉预测 |

## 3. 世界模型的数学基础

### 3.1 马尔可夫决策过程 (MDP)

MDP 由五元组 $(S, A, P, R, \gamma)$ 定义：

- $S$: 状态空间
- $A$: 动作空间
- $P(s'|s, a)$: 转移概率
- $R(s, a)$: 奖励函数
- $\gamma$: 折扣因子

### 3.2 世界模型的概率解释

```
p(s_{1:T}, a_{1:T}, r_{1:T}) = p(s_1) ∏ p(s_t|s_{t-1}, a_{t-1}) p(a_t|s_{1:t-1}) p(r_t|s_t, a_t)
```

世界模型学习:
- **转移模型**: $p(s_t|s_{t-1}, a_{t-1})$
- **奖励模型**: $p(r_t|s_t, a_t)$

### 3.3 隐空间世界模型

```
观测 o_t ──(编码器)──> 隐状态 z_t
                        │
           ┌────────────┼────────────┐
           │            │            │
      (转移模型)    (奖励模型)    (表征模型)
           │            │            │
           v            v            v
        z_{t+1}       r_t         重建 o_t
```

## 4. 世界模型架构

### 4.1 经典架构: World Models (2018)

```python
import torch
import torch.nn as nn

class WorldModel(nn.Module):
    """
    经典World Models架构
    VAE(观测) + MDN-RNN(隐空间动态)
    """
    def __init__(self, obs_dim, z_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.z_dim = z_dim
        
        # VAE编码器
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, z_dim * 2)  # mu, logvar
        )
        
        # VAE解码器
        self.decoder = nn.Sequential(
            nn.Linear(z_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, obs_dim)
        )
        
        # MDN-RNN (混合密度网络)
        self.rnn = nn.LSTM(z_dim + action_dim, hidden_dim, num_layers=2)
        self.mdn = nn.Linear(hidden_dim, z_dim * 3)  # mixture components
        
    def forward(self, obs, action, hidden=None):
        # 编码观测
        z_mu, z_logvar = torch.chunk(self.encoder(obs), 2, dim=-1)
        z = z_mu + torch.randn_like(z_mu) * torch.exp(0.5 * z_logvar)
        
        # RNN处理序列
        rnn_input = torch.cat([z, action], dim=-1)
        rnn_out, hidden = self.rnn(rnn_input, hidden)
        
        # 预测下一隐状态
        z_next_params = self.mdn(rnn_out)
        
        return z_next_params, z_mu, z_logvar, hidden
```

### 4.2 Dreamer架构

```python
class DreamerWorldModel(nn.Module):
    """
    Dreamer的世界模型
    包含: 编码器、转移模型、奖励模型、价值模型
    """
    def __init__(self, obs_channels, action_dim, latent_dim=30, hidden=200):
        super().__init__()
        self.latent_dim = latent_dim
        
        # 观测编码器
        self.encoder = nn.Sequential(
            nn.Conv2d(obs_channels, 32, 4, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, 4, stride=2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 4 * 4, hidden),
            nn.ReLU()
        )
        
        # 隐空间先验/后验
        self.latent_embed = nn.Linear(hidden, latent_dim * 2)
        
        # 转移模型 (RSSM)
        self.trans_deter = nn.GRU(latent_dim + action_dim, hidden)
        self.trans_stoch = nn.Linear(hidden, latent_dim * 2)
        
        # 奖励模型
        self.reward_head = nn.Sequential(
            nn.Linear(hidden + latent_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, 1)
        )
        
        # 解码器
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim + hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, obs_channels * 64 * 64)
        )
        
    def imagine(self, latent, action, horizon):
        """
        想象 rollout
        """
        imagined_trajs = []
        hidden = None
        
        for _ in range(horizon):
            deter = self.trans_deter(torch.cat([latent, action], dim=-1))
            stoch_params = self.trans_stoch(deter)
            stoch = stoch_params[:, :self.latent_dim]
            
            imagined_trajs.append(stoch)
            latent = stoch.detach()
            
        return torch.stack(imagined_trajs, dim=1)
```

## 5. 世界模型的训练

### 5.1 训练目标

$$\mathcal{L}_{world} = \mathcal{L}_{recon} + \mathcal{L}_{KL} + \mathcal{L}_{reward}$$

```python
def compute_world_model_loss(model, batch):
    """
    计算世界模型损失
    """
    obs, action, reward, next_obs = batch
    
    # 编码当前观测
    h = model.encoder(obs)
    z_params = model.latent_embed(h)
    z = z_params[:, :model.latent_dim]
    
    # 转移预测
    deter = model.trans_deter(torch.cat([z, action], dim=-1))
    z_next_params = model.trans_stoch(deter)
    z_next = z_next_params[:, :model.latent_dim]
    
    # 重建
    recon = model.decoder(torch.cat([z, deter], dim=-1))
    
    # 奖励预测
    pred_reward = model.reward_head(torch.cat([z, deter], dim=-1))
    
    # 损失
    recon_loss = F.mse_loss(recon, obs)
    reward_loss = F.mse_loss(pred_reward, reward)
    kl_loss = kl_divergence(z, z_next)
    
    total_loss = recon_loss + reward_loss + 0.1 * kl_loss
    
    return total_loss
```

### 5.2 基于模型的强化学习

```
┌─────────────────────────────────────────────────────────┐
│              MBRL (Model-Based RL) 流程                  │
│                                                         │
│   1. 收集数据    2. 学习世界模型   3. 想象 rollout      │
│   ─────────>   ──────────────>   ──────────────>       │
│   真实环境         模型训练          策略优化            │
│                                                         │
│   4. 执行策略    5. 评估            6. 返回步骤1         │
│   ─────────>   ──────────────>   ──────────────>       │
└─────────────────────────────────────────────────────────┘
```

## 6. 世界模型的应用

### 6.1 在物理AI中的应用

| 应用 | 描述 |
|------|------|
| 机器人控制 | 学习机器人与环境交互 |
| 动作预测 | 预测动作执行结果 |
| 规划 | 在隐空间进行规划 |
| 零样本泛化 | 想象未见过的场景 |

### 6.2 典型应用示例

```python
class RobotWorldModel:
    """
    机器人世界模型示例
    用于预测机器人动作执行结果
    """
    def __init__(self, state_dim, action_dim):
        self.model = build_world_model(state_dim, action_dim)
        
    def predict_next_state(self, state, action):
        """
        预测下一状态
        """
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state)
            action_tensor = torch.FloatTensor(action)
            next_state = self.model.predict(state_tensor, action_tensor)
        return next_state.numpy()
    
    def plan(self, start_state, goal_state, max_steps=100):
        """
        在隐空间进行规划
        """
        # 简化版: 随机采样规划
        best_plan = None
        best_cost = float('inf')
        
        for _ in range(100):
            plan = self.random_plan(start_state, max_steps)
            cost = self.estimate_cost(plan, goal_state)
            if cost < best_cost:
                best_cost = cost
                best_plan = plan
                
        return best_plan
```

## 7. 总结与展望

```
┌────────────────────────────────────────────────────────┐
│                    世界模型核心要点                       │
├────────────────────────────────────────────────────────┤
│  ✓ 状态表征: 将高维感知映射到低维隐空间                   │
│  ✓ 转移预测: 学习环境动态 p(s'|s,a)                     │
│  ✓ 奖励预测: 学习奖励信号 p(r|s,a)                       │
│  ✓ 想象推演: 在隐空间进行规划与决策                       │
├────────────────────────────────────────────────────────┤
│  挑战:                                                   │
│  - 组合复杂度                                            │
│  - 稀疏奖励                                              │
│  - 部分可观测                                            │
│  - 泛化能力                                              │
└────────────────────────────────────────────────────────┘
```

## 8. 扩展阅读

- Ha & Schmidhuber (2018). "World Models"
- Hafner et al. (2019). "Learning Latent Dynamics for Planning"
- Hafner et al. (2020). "Dream to Control: Learning Behaviors by Latent Imagination"

---

*下一章将介绍神经网络世界模型的进阶内容与最新进展。*
